{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Definitive Guide to Decision Trees & Gradient Boosting\n",
    "## XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** 2026  \n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "This notebook is your complete guide to understanding tree-based machine learning models. We start from the very basics - what a decision tree is and how it makes decisions - and build up to mastering the three most powerful gradient boosting frameworks: XGBoost, LightGBM, and CatBoost. By the end, you'll understand not just *how* to use these tools, but *why* they work and *when* to choose each one.\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **[Foundations - What Is a Decision Tree?](#section-1)**\n",
    "2. **[From One Tree to Many - Ensemble Methods](#section-2)**\n",
    "3. **[XGBoost Deep Dive](#section-3)**\n",
    "4. **[LightGBM Deep Dive](#section-4)**\n",
    "5. **[CatBoost Deep Dive](#section-5)**\n",
    "6. **[Scikit-learn's Gradient Boosting](#section-6)**\n",
    "7. **[Head-to-Head Comparison](#section-7)**\n",
    "8. **[Feature Importance & Explainability](#section-8)**\n",
    "9. **[Practical Tips & Common Pitfalls](#section-9)**\n",
    "10. **[Quick Reference Cheat Sheet](#section-10)**\n",
    "11. **[Classification Example (Bonus)](#section-11)**\n",
    "12. **[References & Further Reading](#references)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup & Dependencies\n",
    "\n",
    "Run this cell first to install all required libraries. This notebook uses Python 3.10+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install scikit-learn xgboost lightgbm catboost matplotlib seaborn pandas numpy shap graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, log_loss, roc_curve\n",
    "\n",
    "# Boosting frameworks\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Consistent color palette for frameworks\n",
    "COLORS = {\n",
    "    'XGBoost': '#3498db',      # Blue\n",
    "    'LightGBM': '#2ecc71',     # Green  \n",
    "    'CatBoost': '#f39c12',     # Gold\n",
    "    'sklearn_HGBR': '#e74c3c', # Red\n",
    "    'RandomForest': '#9b59b6', # Purple\n",
    "    'DecisionTree': '#1abc9c'  # Teal\n",
    "}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"All libraries loaded successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='section-1'></a>\n",
    "# Section 1: Foundations - What Is a Decision Tree?\n",
    "\n",
    "Before we dive into complex boosting algorithms, let's build a solid understanding of the building block: **the decision tree**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The \"20 Questions\" Analogy\n",
    "\n",
    "Imagine you're playing a game of 20 Questions. Someone thinks of an animal, and you try to guess it by asking yes/no questions:\n",
    "\n",
    "- \"Is it a mammal?\" \u2192 Yes\n",
    "- \"Is it larger than a cat?\" \u2192 Yes  \n",
    "- \"Does it live in water?\" \u2192 No\n",
    "- \"Does it have stripes?\" \u2192 Yes\n",
    "- \"Is it a tiger?\" \u2192 Yes!\n",
    "\n",
    "**A decision tree works exactly like this.** It asks a series of questions about your data, and each answer leads to either another question or a final prediction.\n",
    "\n",
    "The key insight: **some questions are better than others**. Asking \"Is it a mammal?\" splits animals into two useful groups. Asking \"Does it have exactly 4,721 hairs?\" is useless.\n",
    "\n",
    "Decision trees learn to ask the *best* questions - the ones that most effectively separate the data into distinct groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Your First Decision Tree\n",
    "\n",
    "Before diving into theory, let's see what we're building. Here's a decision tree trained on a simple classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D dataset for visualization\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate two clusters\n",
    "X_simple, y_simple = make_blobs(n_samples=200, centers=2, n_features=2, \n",
    "                                 random_state=RANDOM_STATE, cluster_std=2.0)\n",
    "\n",
    "# Train a shallow decision tree\n",
    "simple_tree = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
    "simple_tree.fit(X_simple, y_simple)\n",
    "\n",
    "# Visualize the tree structure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: The tree itself\n",
    "plot_tree(simple_tree, feature_names=['Feature 1', 'Feature 2'], \n",
    "          class_names=['Class A', 'Class B'], filled=True, rounded=True,\n",
    "          ax=axes[0], fontsize=10)\n",
    "axes[0].set_title('Decision Tree Structure', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Right: Decision boundary overlaid on data\n",
    "x_min, x_max = X_simple[:, 0].min() - 1, X_simple[:, 0].max() + 1\n",
    "y_min, y_max = X_simple[:, 1].min() - 1, X_simple[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "Z = simple_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "axes[1].scatter(X_simple[:, 0], X_simple[:, 1], c=y_simple, cmap='RdYlBu', \n",
    "                edgecolors='black', s=50)\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].set_title('Decision Boundary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTree Accuracy: {simple_tree.score(X_simple, y_simple):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "**Left plot (Tree Structure):** Each box is a \"node\" where the tree asks a question. The top node asks if Feature 1 \u2264 some value. Based on the answer (yes=left, no=right), we move to the next question, until we reach a leaf node with a prediction.\n",
    "\n",
    "**Right plot (Decision Boundary):** The shaded regions show where the tree predicts each class. Notice the boundaries are **axis-aligned rectangles** - that's because each split is on a single feature at a time. Trees can't draw diagonal lines!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 How Splits Work: The Core Idea\n",
    "\n",
    "At every node, the tree asks: **\"Which question best separates my data?\"**\n",
    "\n",
    "But what does \"best\" mean? We need a way to measure how \"mixed\" or \"pure\" a group is. If all samples in a node belong to the same class, it's perfectly pure. If it's 50/50, it's maximally mixed.\n",
    "\n",
    "Let's visualize this with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the concept of splitting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Create sample data for visualization\n",
    "np.random.seed(42)\n",
    "n_points = 40\n",
    "\n",
    "# Class A: mostly on the left\n",
    "class_a_x = np.random.normal(2, 0.8, n_points//2)\n",
    "class_a_y = np.random.uniform(0, 4, n_points//2)\n",
    "\n",
    "# Class B: mostly on the right  \n",
    "class_b_x = np.random.normal(4, 0.8, n_points//2)\n",
    "class_b_y = np.random.uniform(0, 4, n_points//2)\n",
    "\n",
    "# Plot 1: Original mixed data\n",
    "axes[0].scatter(class_a_x, class_a_y, c='#e74c3c', s=100, label='Class A', edgecolors='black')\n",
    "axes[0].scatter(class_b_x, class_b_y, c='#3498db', s=100, label='Class B', edgecolors='black')\n",
    "axes[0].set_title('Before Split: Mixed Data', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 6)\n",
    "axes[0].set_ylim(-0.5, 4.5)\n",
    "axes[0].set_xlabel('Feature Value')\n",
    "\n",
    "# Plot 2: Bad split (horizontal)\n",
    "axes[1].scatter(class_a_x, class_a_y, c='#e74c3c', s=100, edgecolors='black')\n",
    "axes[1].scatter(class_b_x, class_b_y, c='#3498db', s=100, edgecolors='black')\n",
    "axes[1].axhline(y=2, color='gray', linestyle='--', linewidth=3, label='Split at y=2')\n",
    "axes[1].fill_between([0, 6], 2, 4.5, alpha=0.2, color='purple')\n",
    "axes[1].fill_between([0, 6], -0.5, 2, alpha=0.2, color='orange')\n",
    "axes[1].set_title('Bad Split: Still Mixed!', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlim(0, 6)\n",
    "axes[1].set_ylim(-0.5, 4.5)\n",
    "axes[1].set_xlabel('Feature Value')\n",
    "\n",
    "# Plot 3: Good split (vertical)\n",
    "axes[2].scatter(class_a_x, class_a_y, c='#e74c3c', s=100, edgecolors='black')\n",
    "axes[2].scatter(class_b_x, class_b_y, c='#3498db', s=100, edgecolors='black')\n",
    "axes[2].axvline(x=3, color='green', linestyle='--', linewidth=3, label='Split at x=3')\n",
    "axes[2].fill_between([0, 3], -0.5, 4.5, alpha=0.2, color='#e74c3c')\n",
    "axes[2].fill_between([3, 6], -0.5, 4.5, alpha=0.2, color='#3498db')\n",
    "axes[2].set_title('Good Split: Mostly Pure!', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlim(0, 6)\n",
    "axes[2].set_ylim(-0.5, 4.5)\n",
    "axes[2].set_xlabel('Feature Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Insight\n",
    "\n",
    "- **Bad split (middle):** Cutting horizontally at y=2 leaves both regions equally mixed - we haven't learned anything useful.\n",
    "- **Good split (right):** Cutting vertically at x=3 creates two regions that are mostly pure - red on the left, blue on the right.\n",
    "\n",
    "The tree evaluates many possible splits and picks the one that creates the purest groups. But how do we measure \"purity\" mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 Splitting Criteria: The Math (with Examples)\n",
    "\n",
    "### For Classification: Gini Impurity\n",
    "\n",
    "The most common measure. Gini impurity tells us: **\"If I randomly pick two samples from this node, how likely are they to be different classes?\"**\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "Where $p_i$ is the proportion of class $i$ in the node.\n",
    "\n",
    "- **Gini = 0:** Perfect purity (all same class)\n",
    "- **Gini = 0.5:** Maximum impurity for binary classification (50/50 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example: Calculate Gini impurity step by step\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WORKED EXAMPLE: Gini Impurity Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario: A node with 8 samples\n",
    "print(\"\\n\ud83d\udce6 Scenario: A node contains 8 samples\")\n",
    "print(\"   - 5 samples are Class A (red)\")\n",
    "print(\"   - 3 samples are Class B (blue)\")\n",
    "\n",
    "n_total = 8\n",
    "n_class_a = 5\n",
    "n_class_b = 3\n",
    "\n",
    "# Step 1: Calculate proportions\n",
    "p_a = n_class_a / n_total\n",
    "p_b = n_class_b / n_total\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Step 1: Calculate proportions\")\n",
    "print(f\"   p(Class A) = {n_class_a}/{n_total} = {p_a:.4f}\")\n",
    "print(f\"   p(Class B) = {n_class_b}/{n_total} = {p_b:.4f}\")\n",
    "\n",
    "# Step 2: Square each proportion\n",
    "print(f\"\\n\ud83d\udcca Step 2: Square each proportion\")\n",
    "print(f\"   p(A)\u00b2 = {p_a:.4f}\u00b2 = {p_a**2:.4f}\")\n",
    "print(f\"   p(B)\u00b2 = {p_b:.4f}\u00b2 = {p_b**2:.4f}\")\n",
    "\n",
    "# Step 3: Sum and subtract from 1\n",
    "gini = 1 - (p_a**2 + p_b**2)\n",
    "print(f\"\\n\ud83d\udcca Step 3: Gini = 1 - (p(A)\u00b2 + p(B)\u00b2)\")\n",
    "print(f\"   Gini = 1 - ({p_a**2:.4f} + {p_b**2:.4f})\")\n",
    "print(f\"   Gini = 1 - {p_a**2 + p_b**2:.4f}\")\n",
    "print(f\"   Gini = {gini:.4f}\")\n",
    "\n",
    "print(f\"\\n\u2705 Final Answer: Gini Impurity = {gini:.4f}\")\n",
    "print(\"   (0 = pure, 0.5 = maximally impure for binary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Gini impurity across different class proportions\n",
    "\n",
    "def gini_impurity(p):\n",
    "    \"\"\"Calculate Gini impurity for binary classification.\"\"\"\n",
    "    return 1 - (p**2 + (1-p)**2)\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Calculate entropy for binary classification.\"\"\"\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "# Generate values\n",
    "p_values = np.linspace(0.001, 0.999, 100)\n",
    "gini_values = [gini_impurity(p) for p in p_values]\n",
    "entropy_values = [entropy(p) for p in p_values]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(p_values, gini_values, linewidth=3, color='#e74c3c', label='Gini Impurity')\n",
    "ax.plot(p_values, entropy_values, linewidth=3, color='#3498db', label='Entropy')\n",
    "\n",
    "# Mark key points\n",
    "ax.scatter([0.5], [gini_impurity(0.5)], color='#e74c3c', s=150, zorder=5, edgecolors='black')\n",
    "ax.scatter([0.5], [entropy(0.5)], color='#3498db', s=150, zorder=5, edgecolors='black')\n",
    "\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.annotate('Maximum impurity\\n(50/50 split)', xy=(0.5, 0.5), xytext=(0.7, 0.35),\n",
    "            fontsize=11, arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "\n",
    "ax.set_xlabel('Proportion of Class A (p)', fontsize=12)\n",
    "ax.set_ylabel('Impurity Score', fontsize=12)\n",
    "ax.set_title('Gini Impurity vs Entropy: How We Measure \"Mixedness\"', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Add annotations\n",
    "ax.text(0.05, 0.1, 'Pure\\n(all Class A)', fontsize=10, ha='center')\n",
    "ax.text(0.95, 0.1, 'Pure\\n(all Class B)', fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insight: Both curves peak at p=0.5 (maximum uncertainty) and are 0 at the edges (perfect purity).\")\n",
    "print(\"   Gini and Entropy usually give similar results. Sklearn uses Gini by default because it's slightly faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Classification: Entropy (Information Gain)\n",
    "\n",
    "An alternative measure from information theory. Entropy answers: **\"How many yes/no questions do I need to identify a random sample's class?\"**\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "- **Entropy = 0:** Perfect purity\n",
    "- **Entropy = 1:** Maximum impurity for binary classification\n",
    "\n",
    "When we split, we calculate the **Information Gain** = Entropy(parent) - weighted average of Entropy(children)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example: Information Gain from a split\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WORKED EXAMPLE: Information Gain from a Split\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\ud83d\udce6 Before Split (Parent Node): 8 samples\")\n",
    "print(\"   - 5 Class A, 3 Class B\")\n",
    "\n",
    "# Parent entropy\n",
    "p_a_parent = 5/8\n",
    "p_b_parent = 3/8\n",
    "entropy_parent = -p_a_parent * np.log2(p_a_parent) - p_b_parent * np.log2(p_b_parent)\n",
    "print(f\"   Entropy(Parent) = {entropy_parent:.4f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udce6 After Split:\")\n",
    "print(\"   Left Child: 4 samples (4 Class A, 0 Class B) - Pure!\")\n",
    "print(\"   Right Child: 4 samples (1 Class A, 3 Class B)\")\n",
    "\n",
    "# Left child entropy (pure)\n",
    "entropy_left = 0  # All same class\n",
    "print(f\"\\n   Entropy(Left) = {entropy_left:.4f} (pure node)\")\n",
    "\n",
    "# Right child entropy\n",
    "p_a_right = 1/4\n",
    "p_b_right = 3/4\n",
    "entropy_right = -p_a_right * np.log2(p_a_right) - p_b_right * np.log2(p_b_right)\n",
    "print(f\"   Entropy(Right) = {entropy_right:.4f}\")\n",
    "\n",
    "# Weighted average\n",
    "weight_left = 4/8\n",
    "weight_right = 4/8\n",
    "weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "print(f\"\\n\ud83d\udcca Weighted Entropy of Children:\")\n",
    "print(f\"   = (4/8) \u00d7 {entropy_left:.4f} + (4/8) \u00d7 {entropy_right:.4f}\")\n",
    "print(f\"   = {weighted_entropy:.4f}\")\n",
    "\n",
    "# Information gain\n",
    "info_gain = entropy_parent - weighted_entropy\n",
    "print(f\"\\n\u2705 Information Gain = {entropy_parent:.4f} - {weighted_entropy:.4f} = {info_gain:.4f}\")\n",
    "print(\"\\n\ud83d\udca1 Higher Information Gain = Better Split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Regression: Mean Squared Error (MSE)\n",
    "\n",
    "For regression trees, we predict a number (not a class). The \"impurity\" becomes **variance** - how spread out are the target values?\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n",
    "\n",
    "Where $\\bar{y}$ is the mean of the target values in the node.\n",
    "\n",
    "The tree splits to **minimize variance** in each child node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked example: MSE reduction in regression\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WORKED EXAMPLE: Variance Reduction in Regression Tree\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample data: house prices\n",
    "prices = np.array([200, 220, 250, 350, 380, 400])\n",
    "print(f\"\\n\ud83c\udfe0 House Prices (in $1000s): {prices}\")\n",
    "\n",
    "# Before split\n",
    "mean_all = np.mean(prices)\n",
    "var_all = np.var(prices)\n",
    "print(f\"\\n\ud83d\udce6 Before Split (all 6 houses):\")\n",
    "print(f\"   Mean = ${mean_all:.0f}k\")\n",
    "print(f\"   Variance = {var_all:.2f}\")\n",
    "\n",
    "# Split: first 3 vs last 3\n",
    "left = prices[:3]  # [200, 220, 250]\n",
    "right = prices[3:]  # [350, 380, 400]\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 After Split (at $300k threshold):\")\n",
    "print(f\"   Left (< $300k): {left}, Mean = ${np.mean(left):.0f}k, Var = {np.var(left):.2f}\")\n",
    "print(f\"   Right (\u2265 $300k): {right}, Mean = ${np.mean(right):.0f}k, Var = {np.var(right):.2f}\")\n",
    "\n",
    "# Weighted variance\n",
    "weighted_var = (3/6) * np.var(left) + (3/6) * np.var(right)\n",
    "variance_reduction = var_all - weighted_var\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Weighted Variance After Split: {weighted_var:.2f}\")\n",
    "print(f\"\u2705 Variance Reduction: {var_all:.2f} - {weighted_var:.2f} = {variance_reduction:.2f}\")\n",
    "print(f\"\\n\ud83d\udca1 We reduced variance by {(variance_reduction/var_all)*100:.1f}% with one split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.5 The Overfitting Problem\n",
    "\n",
    "Here's the dark side of decision trees: **they're TOO good at fitting training data.**\n",
    "\n",
    "Given enough depth, a tree can create a leaf for every single training sample - achieving 100% training accuracy but learning nothing generalizable.\n",
    "\n",
    "Let's see this in action with real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "california = fetch_california_housing()\n",
    "X_cal = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y_cal = california.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cal, y_cal, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"California Housing Dataset\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features: {X_cal.shape[1]}\")\n",
    "print(f\"\\nTarget: Median house value (in $100,000s)\")\n",
    "print(f\"Target range: ${y_cal.min()*100:.0f}k - ${y_cal.max()*100:.0f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train trees at different depths and track train vs test error\n",
    "depths = [1, 2, 3, 4, 5, 7, 10, 15, 20, None]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=depth, random_state=RANDOM_STATE)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, tree.predict(X_train)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, tree.predict(X_test)))\n",
    "    \n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "\n",
    "# Convert None to a number for plotting\n",
    "depth_labels = [str(d) if d is not None else '\u221e' for d in depths]\n",
    "x_positions = range(len(depths))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x_positions, train_errors, 'o-', linewidth=2.5, markersize=10, \n",
    "        color='#2ecc71', label='Training Error')\n",
    "ax.plot(x_positions, test_errors, 's-', linewidth=2.5, markersize=10,\n",
    "        color='#e74c3c', label='Test Error')\n",
    "\n",
    "# Highlight the overfitting zone\n",
    "ax.axvspan(5, 9, alpha=0.2, color='red', label='Overfitting Zone')\n",
    "\n",
    "# Best test error point\n",
    "best_idx = np.argmin(test_errors)\n",
    "ax.scatter([best_idx], [test_errors[best_idx]], s=200, color='gold', \n",
    "           edgecolors='black', zorder=5, label=f'Best (depth={depth_labels[best_idx]})')\n",
    "\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(depth_labels)\n",
    "ax.set_xlabel('Tree Depth', fontsize=12)\n",
    "ax.set_ylabel('RMSE (Root Mean Squared Error)', fontsize=12)\n",
    "ax.set_title('The Overfitting Problem: Train vs Test Error', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Results Summary:\")\n",
    "print(f\"   Best test RMSE: {min(test_errors):.4f} at depth={depth_labels[best_idx]}\")\n",
    "print(f\"   Unlimited depth test RMSE: {test_errors[-1]:.4f}\")\n",
    "print(f\"\\n\ud83d\udca1 Key Insight: Training error keeps dropping, but test error rises after depth ~{depth_labels[best_idx]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Graph Shows\n",
    "\n",
    "This is the **classic overfitting curve** - one of the most important concepts in machine learning:\n",
    "\n",
    "1. **Training error** (green) keeps decreasing as trees get deeper - eventually reaching near zero\n",
    "2. **Test error** (red) decreases initially, then **increases** as the tree gets too complex\n",
    "3. The **gap** between training and test error is the \"generalization gap\" - larger gaps mean more overfitting\n",
    "\n",
    "**Why this happens:** A very deep tree memorizes the training data, including its noise and quirks. It becomes a lookup table, not a generalizable model.\n",
    "\n",
    "**This is exactly why we need ensembles!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.6 Summary: Strengths & Weaknesses of Decision Trees\n",
    "\n",
    "### \u2705 Strengths\n",
    "\n",
    "| Strength | Why It Matters |\n",
    "|----------|----------------|\n",
    "| **Interpretable** | You can literally see and explain the decision logic |\n",
    "| **No feature scaling needed** | Works with raw features - no normalization required |\n",
    "| **Handles mixed data types** | Works with numbers and categories |\n",
    "| **Captures non-linear relationships** | Unlike linear models |\n",
    "| **Fast to train and predict** | Simple algorithm |\n",
    "| **Handles missing values** | Some implementations can route missing values |\n",
    "\n",
    "### \u274c Weaknesses\n",
    "\n",
    "| Weakness | Why It Matters |\n",
    "|----------|----------------|\n",
    "| **Overfits easily** | As we just saw - deep trees memorize |\n",
    "| **High variance** | Small changes in data \u2192 very different trees |\n",
    "| **Axis-aligned splits only** | Can't capture diagonal relationships efficiently |\n",
    "| **Greedy algorithm** | Finds locally optimal splits, not globally optimal trees |\n",
    "| **Instability** | Remove one sample and the whole tree might change |\n",
    "\n",
    "---\n",
    "\n",
    "**The solution to these weaknesses? Don't use one tree - use hundreds of them.**\n",
    "\n",
    "That's what we'll explore in Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-2\"></a>\n",
    "# Section 2: From One Tree to Many - Ensemble Methods\n",
    "\n",
    "We've seen that single decision trees have a fatal flaw: they overfit. The solution? **Don't rely on one tree - use many.**\n",
    "\n",
    "This section introduces the two main ensemble strategies that revolutionized machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The Problem: High Variance\n",
    "\n",
    "Remember this: a single decision tree has **high variance**. Train it on slightly different data, and you get a completely different tree.\n",
    "\n",
    "Let's see this instability in action:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstrate tree instability - small data changes lead to different trees\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Create base dataset\n",
    "np.random.seed(42)\n",
    "X_demo = np.random.randn(100, 2)\n",
    "y_demo = (X_demo[:, 0] + X_demo[:, 1] > 0).astype(int)\n",
    "\n",
    "# Train 3 trees on slightly different bootstrap samples\n",
    "for i, ax in enumerate(axes):\n",
    "    # Bootstrap sample (sample with replacement)\n",
    "    idx = np.random.choice(len(X_demo), size=len(X_demo), replace=True)\n",
    "    X_boot, y_boot = X_demo[idx], y_demo[idx]\n",
    "    \n",
    "    # Train tree\n",
    "    tree = DecisionTreeClassifier(max_depth=4, random_state=i)\n",
    "    tree.fit(X_boot, y_boot)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=\"RdYlBu\")\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo, cmap=\"RdYlBu\", \n",
    "               edgecolors=\"black\", s=30)\n",
    "    ax.set_title(f\"Tree {i+1}: Different Bootstrap Sample\", fontweight=\"bold\")\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle(\"Same Data, Different Samples \u2192 Different Trees\\!\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Each tree learned a different decision boundary from the same underlying data\\!\")\n",
    "print(\"   This is HIGH VARIANCE - the model is unstable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wisdom of Crowds\n",
    "\n",
    "Here's the key insight: **averaging many unstable predictions gives a stable result**.\n",
    "\n",
    "Think of it like asking 100 people to guess how many jellybeans are in a jar. Individual guesses vary wildly, but the average is often remarkably accurate.\n",
    "\n",
    "This is the foundation of ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize how averaging reduces variance\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Generate predictions from multiple trees\n",
    "np.random.seed(42)\n",
    "n_trees_list = [1, 5, 20, 100]\n",
    "\n",
    "for idx, n_trees in enumerate(n_trees_list):\n",
    "    # Train multiple trees and average their predictions\n",
    "    predictions = np.zeros((200, 200))\n",
    "    \n",
    "    for t in range(n_trees):\n",
    "        # Bootstrap sample\n",
    "        boot_idx = np.random.choice(len(X_demo), size=len(X_demo), replace=True)\n",
    "        X_boot, y_boot = X_demo[boot_idx], y_demo[boot_idx]\n",
    "        \n",
    "        # Train tree\n",
    "        tree = DecisionTreeClassifier(max_depth=4, random_state=t*100)\n",
    "        tree.fit(X_boot, y_boot)\n",
    "        \n",
    "        # Accumulate predictions\n",
    "        xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "        predictions += tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_pred = predictions / n_trees\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    im = ax.contourf(xx, yy, avg_pred, levels=20, cmap=\"RdYlBu\", alpha=0.8)\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo, cmap=\"RdYlBu\", \n",
    "               edgecolors=\"black\", s=20)\n",
    "    ax.set_title(f\"{n_trees} Tree(s)\", fontweight=\"bold\", fontsize=12)\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle(\"Averaging More Trees \u2192 Smoother, More Stable Predictions\", \n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Notice how the decision boundary becomes smoother as we add more trees\\!\")\n",
    "print(\"   The jagged, overfit patterns of single trees get averaged out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 Bagging: Bootstrap Aggregating\n",
    "\n",
    "**Bagging** (Bootstrap AGGregatING) is the first ensemble strategy:\n",
    "\n",
    "1. **Bootstrap**: Create multiple training sets by sampling with replacement\n",
    "2. **Train**: Build a tree on each bootstrap sample (in parallel)\n",
    "3. **Aggregate**: Average predictions (regression) or vote (classification)\n",
    "\n",
    "The key insight: each tree sees slightly different data, so they make different mistakes. Averaging cancels out individual errors."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visual explanation of bagging\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create a visual diagram\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Original data\n",
    "ax.add_patch(plt.Rectangle((0.5, 4), 2, 1.5, facecolor=\"#3498db\", edgecolor=\"black\", linewidth=2))\n",
    "ax.text(1.5, 4.75, \"Original\\nData\", ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Arrows to bootstrap samples\n",
    "for i, y_pos in enumerate([5, 3, 1]):\n",
    "    ax.annotate(\"\", xy=(3.5, y_pos + 0.5), xytext=(2.5, 4.5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "\n",
    "# Bootstrap samples\n",
    "colors = [\"#e74c3c\", \"#2ecc71\", \"#9b59b6\"]\n",
    "for i, (y_pos, color) in enumerate(zip([4.5, 2.5, 0.5], colors)):\n",
    "    ax.add_patch(plt.Rectangle((3.5, y_pos), 2, 1, facecolor=color, edgecolor=\"black\", linewidth=2, alpha=0.7))\n",
    "    ax.text(4.5, y_pos + 0.5, f\"Bootstrap {i+1}\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "\n",
    "# Arrows to trees\n",
    "for i, y_pos in enumerate([5, 3, 1]):\n",
    "    ax.annotate(\"\", xy=(6.5, y_pos + 0.5 - 0.5), xytext=(5.5, y_pos + 0.5 - 0.5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "\n",
    "# Trees\n",
    "for i, (y_pos, color) in enumerate(zip([4.5, 2.5, 0.5], colors)):\n",
    "    # Tree shape\n",
    "    triangle = plt.Polygon([(7.5, y_pos + 1), (6.7, y_pos + 0.2), (8.3, y_pos + 0.2)], \n",
    "                          facecolor=color, edgecolor=\"black\", linewidth=2, alpha=0.7)\n",
    "    ax.add_patch(triangle)\n",
    "    ax.add_patch(plt.Rectangle((7.35, y_pos), 0.3, 0.2, facecolor=\"#8B4513\", edgecolor=\"black\"))\n",
    "    ax.text(7.5, y_pos + 0.5, f\"Tree {i+1}\", ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "# Arrows to aggregation\n",
    "for i, y_pos in enumerate([5, 3, 1]):\n",
    "    ax.annotate(\"\", xy=(9.5, 3), xytext=(8.5, y_pos + 0.5 - 0.5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "\n",
    "# Aggregation box\n",
    "ax.add_patch(plt.Rectangle((9.5, 2), 2.5, 2, facecolor=\"#f39c12\", edgecolor=\"black\", linewidth=2))\n",
    "ax.text(10.75, 3, \"Average\\n(Regression)\\nor Vote\\n(Classification)\", \n",
    "        ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Arrow to final prediction\n",
    "ax.annotate(\"\", xy=(13, 3), xytext=(12, 3),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "\n",
    "# Final prediction\n",
    "ax.add_patch(plt.Circle((13.5, 3), 0.5, facecolor=\"#1abc9c\", edgecolor=\"black\", linewidth=2))\n",
    "ax.text(13.5, 3, \"Final\\nPred\", ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "# Title\n",
    "ax.set_title(\"Bagging: Train Trees in PARALLEL, Then Combine\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "# Labels\n",
    "ax.text(1.5, 3.5, \"Step 1:\\nBootstrap\", ha=\"center\", fontsize=10, color=\"gray\")\n",
    "ax.text(7.5, -0.3, \"Step 2: Train Trees\\n(independently, in parallel)\", ha=\"center\", fontsize=10, color=\"gray\")\n",
    "ax.text(10.75, 1.5, \"Step 3:\\nAggregate\", ha=\"center\", fontsize=10, color=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests: Bagging + Feature Randomization\n",
    "\n",
    "**Random Forests** take bagging one step further. In addition to training on different bootstrap samples, each tree only considers a **random subset of features** at each split.\n",
    "\n",
    "Why? If one feature is very strong, all trees would use it first, making them too similar. Feature randomization forces diversity, which improves the ensemble.\n",
    "\n",
    "Key parameters:\n",
    "- : Number of trees (more = better, until diminishing returns)\n",
    "- : Features to consider at each split (typically \u221an for classification, n/3 for regression)\n",
    "- : Tree depth (often unlimited in Random Forests)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare single tree vs Random Forest on California Housing\n",
    "print(\"Training Single Tree vs Random Forest on California Housing...\\n\")\n",
    "\n",
    "# Single tree (best depth from earlier)\n",
    "single_tree = DecisionTreeRegressor(max_depth=7, random_state=RANDOM_STATE)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_tree_rmse = np.sqrt(mean_squared_error(y_test, single_tree.predict(X_test)))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, rf.predict(X_test)))\n",
    "\n",
    "# Results comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "models = [\"Single Tree\\n(depth=7)\", \"Random Forest\\n(100 trees)\"]\n",
    "rmses = [single_tree_rmse, rf_rmse]\n",
    "colors = [COLORS[\"DecisionTree\"], COLORS[\"RandomForest\"]]\n",
    "\n",
    "bars = axes[0].bar(models, rmses, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "axes[0].set_ylabel(\"RMSE (lower is better)\", fontsize=12)\n",
    "axes[0].set_title(\"Single Tree vs Random Forest Performance\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, rmse in zip(bars, rmses):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f\"{rmse:.4f}\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Improvement annotation\n",
    "improvement = (single_tree_rmse - rf_rmse) / single_tree_rmse * 100\n",
    "axes[0].annotate(f\"{improvement:.1f}% better\\!\", xy=(1, rf_rmse), xytext=(1.3, rf_rmse + 0.1),\n",
    "                fontsize=11, color=\"green\", fontweight=\"bold\",\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=\"green\"))\n",
    "\n",
    "# Actual vs Predicted scatter\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "axes[1].scatter(y_test, y_pred_rf, alpha=0.5, c=COLORS[\"RandomForest\"], edgecolors=\"white\", s=30)\n",
    "axes[1].plot([0, 5], [0, 5], \"r--\", linewidth=2, label=\"Perfect Prediction\")\n",
    "axes[1].set_xlabel(\"Actual Price (k)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Price (k)\", fontsize=12)\n",
    "axes[1].set_title(\"Random Forest: Actual vs Predicted\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 5.5)\n",
    "axes[1].set_ylim(0, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\ud83d\udcca Results:\")\n",
    "print(f\"   Single Tree RMSE: {single_tree_rmse:.4f}\")\n",
    "print(f\"   Random Forest RMSE: {rf_rmse:.4f}\")\n",
    "print(f\"   Improvement: {improvement:.1f}%\")\n",
    "print(f\"\\n\ud83d\udca1 Random Forests reduce variance by averaging many diverse trees\\!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 Boosting: Sequential Error Correction\n",
    "\n",
    "Boosting takes a completely different approach from bagging:\n",
    "\n",
    "| Bagging (Random Forest) | Boosting |\n",
    "|-------------------------|----------|\n",
    "| Trees trained **in parallel** | Trees trained **sequentially** |\n",
    "| Each tree is independent | Each tree corrects previous errors |\n",
    "| Reduces **variance** | Reduces **bias** |\n",
    "| Uses full-depth trees | Uses shallow trees (\"stumps\") |\n",
    "\n",
    "The boosting philosophy: **\"Each new tree focuses on the mistakes of the previous ensemble.\"**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visual explanation of boosting\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 5)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Initial prediction\n",
    "ax.add_patch(plt.Rectangle((0.5, 2), 2, 1.5, facecolor=\"#3498db\", edgecolor=\"black\", linewidth=2))\n",
    "ax.text(1.5, 2.75, \"Start:\\nMean(y)\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Tree 1\n",
    "ax.annotate(\"\", xy=(3.5, 2.75), xytext=(2.5, 2.75), arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "triangle1 = plt.Polygon([(4.5, 4), (3.7, 2.5), (5.3, 2.5)], facecolor=\"#e74c3c\", edgecolor=\"black\", linewidth=2)\n",
    "ax.add_patch(triangle1)\n",
    "ax.text(4.5, 3, \"Tree 1\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "ax.text(4.5, 1.8, \"Fits residuals\\n(errors)\", ha=\"center\", va=\"center\", fontsize=9, color=\"gray\")\n",
    "\n",
    "# Plus sign\n",
    "ax.text(5.8, 2.75, \"+\", fontsize=20, fontweight=\"bold\", ha=\"center\")\n",
    "\n",
    "# Tree 2\n",
    "ax.annotate(\"\", xy=(6.5, 2.75), xytext=(6.2, 2.75), arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=2))\n",
    "triangle2 = plt.Polygon([(7.5, 4), (6.7, 2.5), (8.3, 2.5)], facecolor=\"#2ecc71\", edgecolor=\"black\", linewidth=2)\n",
    "ax.add_patch(triangle2)\n",
    "ax.text(7.5, 3, \"Tree 2\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "ax.text(7.5, 1.8, \"Fits remaining\\nerrors\", ha=\"center\", va=\"center\", fontsize=9, color=\"gray\")\n",
    "\n",
    "# Plus sign\n",
    "ax.text(8.8, 2.75, \"+\", fontsize=20, fontweight=\"bold\", ha=\"center\")\n",
    "\n",
    "# Tree 3\n",
    "triangle3 = plt.Polygon([(10.5, 4), (9.7, 2.5), (11.3, 2.5)], facecolor=\"#9b59b6\", edgecolor=\"black\", linewidth=2)\n",
    "ax.add_patch(triangle3)\n",
    "ax.text(10.5, 3, \"Tree 3\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "ax.text(10.5, 1.8, \"Fits remaining\\nerrors\", ha=\"center\", va=\"center\", fontsize=9, color=\"gray\")\n",
    "\n",
    "# Plus sign and dots\n",
    "ax.text(11.8, 2.75, \"+ ...\", fontsize=16, fontweight=\"bold\", ha=\"center\")\n",
    "\n",
    "# Equals sign\n",
    "ax.text(13, 2.75, \"=\", fontsize=20, fontweight=\"bold\", ha=\"center\")\n",
    "\n",
    "# Final prediction\n",
    "ax.add_patch(plt.Circle((14.5, 2.75), 0.7, facecolor=\"#f39c12\", edgecolor=\"black\", linewidth=2))\n",
    "ax.text(14.5, 2.75, \"Final\\nPred\", ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Title\n",
    "ax.set_title(\"Boosting: Train Trees SEQUENTIALLY, Each Correcting Previous Errors\", \n",
    "             fontsize=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "# Key insight\n",
    "ax.text(8, 0.5, \"Each tree is SMALL (weak learner), but combined they're powerful\\!\", \n",
    "        ha=\"center\", fontsize=11, style=\"italic\", color=\"#555\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.4 Gradient Boosting from Scratch\n",
    "\n",
    "Let's walk through exactly how gradient boosting works, step by step, with actual numbers.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. **Initialize** with a constant prediction (usually the mean of y)\n",
    "2. **Calculate residuals**: \n",
    "3. **Fit a tree** to predict the residuals\n",
    "4. **Update predictions**: \n",
    "5. **Repeat** steps 2-4 for N iterations\n",
    "\n",
    "The \"gradient\" part: residuals ARE the negative gradient of MSE loss\\!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Gradient Boosting from scratch - step by step with a toy example\n",
    "print(\"=\"*70)\n",
    "print(\"GRADIENT BOOSTING FROM SCRATCH: A Worked Example\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Toy dataset: 6 houses with prices\n",
    "X_toy = np.array([[1], [2], [3], [4], [5], [6]])  # Feature: size\n",
    "y_toy = np.array([1.5, 1.8, 3.2, 3.5, 4.8, 5.1])  # Target: price\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset: 6 houses\")\n",
    "print(f\"   Size (X): {X_toy.flatten()}\")\n",
    "print(f\"   Price (y): {y_toy}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.5\n",
    "n_iterations = 3\n",
    "\n",
    "print(f\"\\n\u2699\ufe0f Settings: learning_rate={learning_rate}, iterations={n_iterations}\")\n",
    "\n",
    "# Step 0: Initialize with mean\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 0: Initialize predictions with mean(y)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "initial_pred = np.mean(y_toy)\n",
    "predictions = np.full_like(y_toy, initial_pred)\n",
    "\n",
    "print(f\"   mean(y) = {initial_pred:.2f}\")\n",
    "print(f\"   Initial predictions: {predictions}\")\n",
    "\n",
    "# Store history for plotting\n",
    "pred_history = [predictions.copy()]\n",
    "residual_history = []\n",
    "\n",
    "# Boosting iterations\n",
    "trees = []\n",
    "for iteration in range(n_iterations):\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"ITERATION {iteration + 1}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Calculate residuals\n",
    "    residuals = y_toy - predictions\n",
    "    residual_history.append(residuals.copy())\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc9 Step 1: Calculate residuals (actual - predicted)\")\n",
    "    print(f\"   Actual:      {y_toy}\")\n",
    "    print(f\"   Predicted:   {np.round(predictions, 2)}\")\n",
    "    print(f\"   Residuals:   {np.round(residuals, 2)}\")\n",
    "    \n",
    "    # Step 2: Fit a tree to residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=1, random_state=iteration)\n",
    "    tree.fit(X_toy, residuals)\n",
    "    trees.append(tree)\n",
    "    \n",
    "    tree_predictions = tree.predict(X_toy)\n",
    "    print(f\"\\n\ud83c\udf33 Step 2: Fit a shallow tree to predict residuals\")\n",
    "    print(f\"   Tree predictions: {np.round(tree_predictions, 2)}\")\n",
    "    \n",
    "    # Step 3: Update predictions\n",
    "    update = learning_rate * tree_predictions\n",
    "    predictions = predictions + update\n",
    "    pred_history.append(predictions.copy())\n",
    "    \n",
    "    print(f\"\\n\u2705 Step 3: Update predictions\")\n",
    "    print(f\"   new_pred = old_pred + {learning_rate} \u00d7 tree_pred\")\n",
    "    print(f\"   Update:   {np.round(update, 2)}\")\n",
    "    print(f\"   New pred: {np.round(predictions, 2)}\")\n",
    "    \n",
    "    # Current error\n",
    "    mse = np.mean((y_toy - predictions)**2)\n",
    "    print(f\"\\n   Current MSE: {mse:.4f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Actual:     {y_toy}\")\n",
    "print(f\"   Predicted:  {np.round(predictions, 2)}\")\n",
    "print(f\"   Final MSE:  {np.mean((y_toy - predictions)**2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize how predictions improve with each iteration\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "titles = [\"Initial (Mean)\", \"After Tree 1\", \"After Tree 2\", \"After Tree 3\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#9b59b6\"]\n",
    "\n",
    "for i, (ax, preds, title, color) in enumerate(zip(axes, pred_history, titles, colors)):\n",
    "    # Plot actual values\n",
    "    ax.scatter(X_toy, y_toy, s=150, c=\"black\", label=\"Actual\", zorder=5, edgecolors=\"white\", linewidth=2)\n",
    "    \n",
    "    # Plot predictions as line\n",
    "    ax.plot(X_toy, preds, \"o-\", color=color, linewidth=3, markersize=10, label=\"Predicted\")\n",
    "    \n",
    "    # Show residuals as vertical lines\n",
    "    for x, y_actual, y_pred in zip(X_toy.flatten(), y_toy, preds):\n",
    "        ax.plot([x, x], [y_actual, y_pred], \"--\", color=\"gray\", alpha=0.5, linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel(\"Size\")\n",
    "    ax.set_ylabel(\"Price\")\n",
    "    ax.set_title(title, fontweight=\"bold\", fontsize=12)\n",
    "    ax.legend(loc=\"upper left\", fontsize=9)\n",
    "    ax.set_ylim(0, 6)\n",
    "    \n",
    "    # Add MSE\n",
    "    mse = np.mean((y_toy - preds)**2)\n",
    "    ax.text(0.95, 0.05, f\"MSE: {mse:.3f}\", transform=ax.transAxes, \n",
    "            ha=\"right\", fontsize=10, color=color, fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"Gradient Boosting: Predictions Improve With Each Tree\", \n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Notice how the predictions (colored line) get closer to actual values (black dots) with each tree\\!\")\n",
    "print(\"   Each tree corrects the errors (gray dashed lines) of the previous prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why \"Gradient\"?\n",
    "\n",
    "The residuals we're fitting are actually the **negative gradient** of the loss function\\!\n",
    "\n",
    "For Mean Squared Error loss:  = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "\n",
    "The gradient with respect to $\\hat{y}$ is: $\\frac{\\partial L}{\\partial \\hat{y}} = -(y - \\hat{y}) = -\\text{residual}$\n",
    "\n",
    "So fitting to residuals = fitting to negative gradients = doing **gradient descent in function space**.\n",
    "\n",
    "This is a profound insight: we're not just fitting residuals - we're optimizing a loss function by adding functions\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.5 A Brief History of Boosting\n",
    "\n",
    "Understanding the evolution helps explain why we have so many frameworks today:\n",
    "\n",
    "| Year | Algorithm | Key Innovation |\n",
    "|------|-----------|----------------|\n",
    "| 1990 | **AdaBoost** | First practical boosting algorithm (for classification) |\n",
    "| 1999 | **Gradient Boosting** | Generalized boosting to any differentiable loss function |\n",
    "| 2014 | **XGBoost** | Regularization + engineering optimizations = Kaggle dominance |\n",
    "| 2017 | **LightGBM** | Leaf-wise growth + faster histogram building |\n",
    "| 2017 | **CatBoost** | Superior categorical feature handling + ordered boosting |\n",
    "\n",
    "Each framework improved on the previous, but they all share the same core gradient boosting idea.\n",
    "\n",
    "In the next sections, we'll deep-dive into XGBoost, LightGBM, and CatBoost to understand exactly what makes each one special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.6 Summary: Bagging vs Boosting\n",
    "\n",
    "| Aspect | Bagging (Random Forest) | Boosting (XGBoost, etc.) |\n",
    "|--------|------------------------|--------------------------|\n",
    "| **Training** | Parallel (independent trees) | Sequential (dependent trees) |\n",
    "| **Tree type** | Deep, complex trees | Shallow, simple trees |\n",
    "| **What it reduces** | Variance | Bias |\n",
    "| **Overfitting risk** | Lower | Higher (needs regularization) |\n",
    "| **Training speed** | Faster (parallelizable) | Slower (sequential) |\n",
    "| **Typical accuracy** | Good | Often better |\n",
    "\n",
    "**When to use what:**\n",
    "- **Random Forest**: When you want something robust with little tuning\n",
    "- **Boosting**: When you want maximum accuracy and are willing to tune\n",
    "\n",
    "In practice, gradient boosting (XGBoost, LightGBM, CatBoost) often wins competitions and achieves state-of-the-art results on tabular data.\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: Section 3 - XGBoost Deep Dive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-3\"></a>\n",
    "# Section 3: XGBoost Deep Dive\n",
    "\n",
    "**XGBoost** (eXtreme Gradient Boosting) was released in 2014 and quickly became the dominant algorithm for structured/tabular data. It won countless Kaggle competitions and became a go-to tool for data scientists.\n",
    "\n",
    "Let's understand what made XGBoost so special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 What XGBoost Added to Gradient Boosting\n",
    "\n",
    "Vanilla gradient boosting existed before XGBoost (sklearn had `GradientBoostingClassifier`). But XGBoost added several key innovations that made it faster and more accurate:\n",
    "\n",
    "### Innovation 1: Regularized Objective Function\n",
    "\n",
    "XGBoost adds penalties to prevent overfitting:\n",
    "\n",
    "$$\\text{Objective} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)$$\n",
    "\n",
    "Where the regularization term is:\n",
    "\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2$$\n",
    "\n",
    "- **T**: Number of leaves in the tree\n",
    "- **w_j**: Weight (prediction value) of leaf j\n",
    "- **\u03b3 (gamma)**: Penalty for adding more leaves (complexity cost)\n",
    "- **\u03bb (lambda)**: L2 regularization on leaf weights\n",
    "\n",
    "**In plain English:** XGBoost penalizes both having too many leaves AND having extreme predictions in any leaf. This prevents the wild, overfit trees that plague vanilla boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Innovation 2: Second-Order Optimization\n",
    "\n",
    "Vanilla gradient boosting only uses the first derivative (gradient) of the loss function.\n",
    "\n",
    "XGBoost also uses the **second derivative (Hessian)**, which provides information about the curvature of the loss surface.\n",
    "\n",
    "**Analogy:** Imagine rolling a ball down a hill.\n",
    "- **Gradient only:** You know which direction is downhill\n",
    "- **Gradient + Hessian:** You also know how steep/curved the hill is, so you know how far to roll\n",
    "\n",
    "This leads to better split decisions and faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Innovation 3: Efficient Split Finding\n",
    "\n",
    "Finding the best split is expensive. For each feature, you need to sort values and try every possible split point.\n",
    "\n",
    "XGBoost offers several algorithms:\n",
    "\n",
    "| Method | How It Works | When to Use |\n",
    "|--------|--------------|-------------|\n",
    "| `exact` | Try every possible split | Small datasets (<10k rows) |\n",
    "| `approx` | Use percentile-based candidates | Medium datasets |\n",
    "| `hist` | Bin values into histograms | Large datasets (default) |\n",
    "\n",
    "The **histogram method** bins continuous features into discrete buckets (e.g., 256 bins), making split finding much faster with minimal accuracy loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize histogram-based split finding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generate sample feature values\n",
    "np.random.seed(42)\n",
    "feature_values = np.concatenate([np.random.normal(2, 0.5, 500), \n",
    "                                  np.random.normal(5, 1, 500)])\n",
    "\n",
    "# Left: Raw values (many possible split points)\n",
    "axes[0].hist(feature_values, bins=100, color=\"#3498db\", edgecolor=\"white\", alpha=0.7)\n",
    "axes[0].set_title(\"Exact Method: Try Every Split Point\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Feature Value\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Add vertical lines for some split candidates\n",
    "for x in np.linspace(0, 8, 20):\n",
    "    axes[0].axvline(x, color=\"red\", alpha=0.3, linewidth=0.5)\n",
    "axes[0].text(0.95, 0.95, f\"~{len(np.unique(feature_values))} possible splits!\", \n",
    "             transform=axes[0].transAxes, ha=\"right\", va=\"top\", fontsize=10, color=\"red\")\n",
    "\n",
    "# Right: Histogram (only bin boundaries as split candidates)\n",
    "n_bins = 16\n",
    "counts, bin_edges, _ = axes[1].hist(feature_values, bins=n_bins, color=\"#2ecc71\", \n",
    "                                      edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_title(f\"Histogram Method: Only {n_bins} Split Points\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Feature Value\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Add vertical lines for bin boundaries\n",
    "for edge in bin_edges:\n",
    "    axes[1].axvline(edge, color=\"red\", linewidth=2, alpha=0.7)\n",
    "axes[1].text(0.95, 0.95, f\"Only {n_bins} splits to try!\", \n",
    "             transform=axes[1].transAxes, ha=\"right\", va=\"top\", fontsize=10, color=\"green\")\n",
    "\n",
    "plt.suptitle(\"Histogram-Based Split Finding: Much Faster with Similar Accuracy\", \n",
    "             fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insight: Histogram method reduces split candidates from thousands to ~256,\")\n",
    "print(\"   making training much faster while barely affecting accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Innovation 4: Built-in Missing Value Handling\n",
    "\n",
    "Real data has missing values. XGBoost handles them elegantly:\n",
    "\n",
    "1. During training, it learns the **optimal direction** to send missing values at each split\n",
    "2. It tries both directions and picks whichever reduces the loss more\n",
    "3. During prediction, missing values follow the learned direction\n",
    "\n",
    "No imputation needed - XGBoost figures it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Innovation 5: Column Subsampling\n",
    "\n",
    "Like Random Forest, XGBoost can randomly select a subset of features when building each tree or each split. This adds regularization and speeds up training.\n",
    "\n",
    "Parameters:\n",
    "- `colsample_bytree`: Features per tree\n",
    "- `colsample_bylevel`: Features per depth level\n",
    "- `colsample_bynode`: Features per split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.2 XGBoost Parameters Explained\n",
    "\n",
    "XGBoost has many parameters. Here's what they do, organized by purpose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Structure Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `max_depth` | Maximum tree depth | 6 | 3-10 |\n",
    "| `min_child_weight` | Minimum sum of instance weights in a leaf | 1 | 1-10 |\n",
    "| `gamma` | Minimum loss reduction to make a split | 0 | 0-5 |\n",
    "\n",
    "**Intuition:**\n",
    "- Higher `max_depth` = more complex trees = more overfitting risk\n",
    "- Higher `min_child_weight` = requires more samples in leaves = less overfitting\n",
    "- Higher `gamma` = requires more improvement to split = simpler trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `reg_alpha` (\u03b1) | L1 regularization on weights | 0 | 0-1 |\n",
    "| `reg_lambda` (\u03bb) | L2 regularization on weights | 1 | 1-10 |\n",
    "\n",
    "**Intuition:**\n",
    "- L1 (alpha) encourages sparsity - some leaves get zero weight\n",
    "- L2 (lambda) shrinks all weights toward zero - prevents extreme predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `subsample` | Fraction of rows per tree | 1.0 | 0.5-1.0 |\n",
    "| `colsample_bytree` | Fraction of features per tree | 1.0 | 0.5-1.0 |\n",
    "| `colsample_bylevel` | Fraction of features per level | 1.0 | 0.5-1.0 |\n",
    "\n",
    "**Intuition:**\n",
    "- Lower values = more randomization = less overfitting\n",
    "- Typically `subsample=0.8` and `colsample_bytree=0.8` work well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `learning_rate` (eta) | Shrinkage factor for each tree | 0.3 | 0.01-0.3 |\n",
    "| `n_estimators` | Number of boosting rounds | 100 | 100-10000 |\n",
    "\n",
    "**Critical insight:** These are coupled!\n",
    "- Lower learning_rate \u2192 need more trees \u2192 usually better results but slower\n",
    "- Always use **early stopping** instead of guessing n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstrate the learning_rate vs n_estimators tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates = [0.3, 0.1, 0.05, 0.01]\n",
    "colors = [\"#e74c3c\", \"#f39c12\", \"#2ecc71\", \"#3498db\"]\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    # Train with early stopping\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=lr,\n",
    "        max_depth=4,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Get learning curve\n",
    "    results = model.evals_result()\n",
    "    test_rmse = np.sqrt(results[\"validation_0\"][\"rmse\"])\n",
    "    \n",
    "    # Plot\n",
    "    axes[0].plot(test_rmse, label=f\"lr={lr} (best at {model.best_iteration})\", \n",
    "                 color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel(\"Boosting Round\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Validation RMSE\", fontsize=12)\n",
    "axes[0].set_title(\"Learning Rate vs Convergence Speed\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 1000)\n",
    "\n",
    "# Bar chart of final performance\n",
    "final_rmses = []\n",
    "best_iters = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=lr,\n",
    "        max_depth=4,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    final_rmses.append(rmse)\n",
    "    best_iters.append(model.best_iteration)\n",
    "\n",
    "x_pos = np.arange(len(learning_rates))\n",
    "bars = axes[1].bar(x_pos, final_rmses, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([f\"lr={lr}\\n({iters} trees)\" for lr, iters in zip(learning_rates, best_iters)])\n",
    "axes[1].set_ylabel(\"Final RMSE\", fontsize=12)\n",
    "axes[1].set_title(\"Lower Learning Rate = More Trees = Often Better\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Add value labels\n",
    "for bar, rmse in zip(bars, final_rmses):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "                f\"{rmse:.4f}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insight: Lower learning rate needs more trees but often achieves better performance.\")\n",
    "print(\"   Always use early_stopping to find the optimal number of trees!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 XGBoost in Practice\n",
    "\n",
    "Let's train XGBoost properly on California Housing with early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train XGBoost with proper early stopping\n",
    "print(\"Training XGBoost on California Housing...\\n\")\n",
    "\n",
    "# Create validation set from training data\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    early_stopping_rounds=50,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "xgb_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "train_time = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\ud83d\udcca XGBoost Results:\")\n",
    "print(f\"   Best iteration: {xgb_model.best_iteration}\")\n",
    "print(f\"   Training time: {train_time:.2f}s\")\n",
    "print(f\"   Test RMSE: {rmse:.4f}\")\n",
    "print(f\"   Test MAE: {mae:.4f}\")\n",
    "print(f\"   Test R\u00b2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize training progress and predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Learning curves\n",
    "results = xgb_model.evals_result()\n",
    "train_rmse = np.sqrt(results[\"validation_0\"][\"rmse\"])\n",
    "val_rmse = np.sqrt(results[\"validation_1\"][\"rmse\"])\n",
    "\n",
    "axes[0].plot(train_rmse, label=\"Training\", color=\"#2ecc71\", linewidth=2)\n",
    "axes[0].plot(val_rmse, label=\"Validation\", color=\"#e74c3c\", linewidth=2)\n",
    "axes[0].axvline(xgb_model.best_iteration, color=\"gray\", linestyle=\"--\", \n",
    "               label=f\"Best iteration ({xgb_model.best_iteration})\")\n",
    "axes[0].set_xlabel(\"Boosting Round\", fontsize=12)\n",
    "axes[0].set_ylabel(\"RMSE\", fontsize=12)\n",
    "axes[0].set_title(\"XGBoost Learning Curves\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Actual vs Predicted\n",
    "axes[1].scatter(y_test, y_pred, alpha=0.5, c=COLORS[\"XGBoost\"], edgecolors=\"white\", s=30)\n",
    "axes[1].plot([0, 5], [0, 5], \"r--\", linewidth=2, label=\"Perfect Prediction\")\n",
    "axes[1].set_xlabel(\"Actual Price ($100k)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Predicted Price ($100k)\", fontsize=12)\n",
    "axes[1].set_title(\"XGBoost: Actual vs Predicted\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 5.5)\n",
    "axes[1].set_ylim(0, 5.5)\n",
    "\n",
    "# Add R\u00b2 to plot\n",
    "axes[1].text(0.05, 0.95, f\"R\u00b2 = {r2:.4f}\", transform=axes[1].transAxes, \n",
    "            fontsize=12, fontweight=\"bold\", va=\"top\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.4 XGBoost Parameter Reference\n",
    "\n",
    "| Parameter | XGBoost Name | Description | Recommended Range |\n",
    "|-----------|--------------|-------------|-------------------|\n",
    "| Learning rate | `learning_rate` / `eta` | Shrinkage per tree | 0.01-0.3 |\n",
    "| Number of trees | `n_estimators` | Boosting rounds | Use early stopping |\n",
    "| Max depth | `max_depth` | Tree depth | 3-10 |\n",
    "| Min child weight | `min_child_weight` | Min samples/leaf | 1-10 |\n",
    "| Subsample | `subsample` | Row sampling | 0.5-1.0 |\n",
    "| Column sample | `colsample_bytree` | Feature sampling | 0.5-1.0 |\n",
    "| Min split loss | `gamma` | Split threshold | 0-5 |\n",
    "| L1 regularization | `reg_alpha` | Lasso penalty | 0-1 |\n",
    "| L2 regularization | `reg_lambda` | Ridge penalty | 1-10 |\n",
    "| Tree method | `tree_method` | Split algorithm | \"hist\" (default) |\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: Section 4 - LightGBM Deep Dive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-4\"></a>\n",
    "# Section 4: LightGBM Deep Dive\n",
    "\n",
    "**LightGBM** (Light Gradient Boosting Machine) was released by Microsoft in 2017. Its main innovation: **speed**. LightGBM can train 10-20x faster than XGBoost on large datasets while achieving similar or better accuracy.\n",
    "\n",
    "Let's understand what makes LightGBM so fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Key Innovation #1: Leaf-Wise Tree Growth\n",
    "\n",
    "This is the biggest conceptual difference from XGBoost:\n",
    "\n",
    "| XGBoost (Level-Wise) | LightGBM (Leaf-Wise) |\n",
    "|---------------------|----------------------|\n",
    "| Grows trees level by level | Grows trees leaf by leaf |\n",
    "| All nodes at depth d before d+1 | Always splits the leaf with max gain |\n",
    "| More balanced trees | Can be asymmetric |\n",
    "| Safer (less overfit risk) | Faster convergence, but more overfit risk |\n",
    "\n",
    "Let's visualize the difference:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize level-wise vs leaf-wise tree growth\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Helper function to draw tree nodes\n",
    "def draw_node(ax, x, y, text, color, size=0.15):\n",
    "    circle = plt.Circle((x, y), size, facecolor=color, edgecolor=\"black\", linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.text(x, y, text, ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "# LEFT: Level-wise (XGBoost style)\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Level-Wise Growth (XGBoost)\\nGrow ALL nodes at each level\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Level 0 (root)\n",
    "draw_node(ax, 2, 3.5, \"1\", \"#3498db\")\n",
    "\n",
    "# Level 1  \n",
    "draw_node(ax, 1, 2.5, \"2\", \"#e74c3c\")\n",
    "draw_node(ax, 3, 2.5, \"3\", \"#e74c3c\")\n",
    "ax.plot([2, 1], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "ax.plot([2, 3], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Level 2\n",
    "draw_node(ax, 0.5, 1.5, \"4\", \"#2ecc71\")\n",
    "draw_node(ax, 1.5, 1.5, \"5\", \"#2ecc71\")\n",
    "draw_node(ax, 2.5, 1.5, \"6\", \"#2ecc71\")\n",
    "draw_node(ax, 3.5, 1.5, \"7\", \"#2ecc71\")\n",
    "ax.plot([1, 0.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "ax.plot([1, 1.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "ax.plot([3, 2.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "ax.plot([3, 3.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Labels for levels\n",
    "ax.text(0.1, 3.5, \"Level 0\", fontsize=10, color=\"gray\")\n",
    "ax.text(0.1, 2.5, \"Level 1\", fontsize=10, color=\"gray\")\n",
    "ax.text(0.1, 1.5, \"Level 2\", fontsize=10, color=\"gray\")\n",
    "\n",
    "# Order annotations\n",
    "ax.annotate(\"\", xy=(1, 3.3), xytext=(2, 3.7), \n",
    "           arrowprops=dict(arrowstyle=\"->\", color=\"blue\", lw=2))\n",
    "\n",
    "# RIGHT: Leaf-wise (LightGBM style)  \n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Leaf-Wise Growth (LightGBM)\\nAlways split BEST leaf\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Root\n",
    "draw_node(ax, 2, 3.5, \"1\", \"#3498db\")\n",
    "\n",
    "# First split (best leaf)\n",
    "draw_node(ax, 1, 2.5, \"2\", \"#e74c3c\")\n",
    "draw_node(ax, 3, 2.5, \"3\", \"#2ecc71\")  # Leaf stays\n",
    "ax.plot([2, 1], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "ax.plot([2, 3], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Second split (node 2 had more gain)\n",
    "draw_node(ax, 0.5, 1.5, \"4\", \"#9b59b6\")\n",
    "draw_node(ax, 1.5, 1.5, \"5\", \"#f39c12\")  \n",
    "ax.plot([1, 0.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "ax.plot([1, 1.5], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Third split (leaf 5 had most gain)\n",
    "draw_node(ax, 1.2, 0.5, \"6\", \"#1abc9c\")\n",
    "draw_node(ax, 1.8, 0.5, \"7\", \"#1abc9c\")\n",
    "ax.plot([1.5, 1.2], [1.35, 0.65], \"k-\", linewidth=2)\n",
    "ax.plot([1.5, 1.8], [1.35, 0.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Note asymmetry\n",
    "ax.text(2, 0.8, \"\u2190 Asymmetric!\", fontsize=10, color=\"red\", fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"Tree Growth Strategies: Level-Wise vs Leaf-Wise\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insight:\")\n",
    "print(\"   Level-wise: Balanced trees, safer but may need more iterations\")\n",
    "print(\"   Leaf-wise: Finds complex patterns faster, but needs regularization to avoid overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_leaves: The Key Parameter\n",
    "\n",
    "Because of leaf-wise growth, **`num_leaves` is LightGBM's primary complexity control**, not `max_depth`.\n",
    "\n",
    "Rule of thumb: `num_leaves \u2248 2^max_depth`\n",
    "\n",
    "| max_depth | Equivalent num_leaves |\n",
    "|-----------|----------------------|\n",
    "| 3 | ~8 |\n",
    "| 5 | ~32 |\n",
    "| 7 | ~128 |\n",
    "| 10 | ~1024 |\n",
    "\n",
    "**Important:** Setting `num_leaves` too high causes overfitting. Start with 31 (default) and tune carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 Key Innovation #2: GOSS (Gradient-based One-Side Sampling)\n",
    "\n",
    "Traditional boosting uses all samples for each tree. GOSS is a clever sampling trick:\n",
    "\n",
    "1. **Keep ALL samples with large gradients** (large errors - these are most informative)\n",
    "2. **Randomly sample from small-gradient samples** (and upweight them to correct the bias)\n",
    "\n",
    "Why it works: Samples with small gradients are already well-predicted. They contribute less to learning, so we can skip most of them.\n",
    "\n",
    "Result: Much faster training with minimal accuracy loss."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize GOSS concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Generate sample gradients\n",
    "np.random.seed(42)\n",
    "gradients = np.concatenate([np.random.exponential(0.1, 800),  # Small gradients\n",
    "                             np.random.exponential(1.0, 200)])  # Large gradients\n",
    "np.random.shuffle(gradients)\n",
    "\n",
    "# Left: Original gradient distribution\n",
    "axes[0].hist(gradients, bins=50, color=\"#3498db\", edgecolor=\"white\", alpha=0.7)\n",
    "axes[0].set_title(\"1. All Gradients\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Gradient Magnitude\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].axvline(np.percentile(gradients, 80), color=\"red\", linestyle=\"--\", \n",
    "               linewidth=2, label=\"Top 20% threshold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Middle: What we keep\n",
    "threshold = np.percentile(gradients, 80)\n",
    "large_grads = gradients[gradients >= threshold]\n",
    "small_grads = gradients[gradients < threshold]\n",
    "\n",
    "# Sample 20% of small gradients\n",
    "sampled_small = np.random.choice(small_grads, size=len(small_grads)//5, replace=False)\n",
    "\n",
    "axes[1].hist(large_grads, bins=30, color=\"#e74c3c\", edgecolor=\"white\", alpha=0.7, label=\"Keep ALL large\")\n",
    "axes[1].hist(sampled_small, bins=30, color=\"#2ecc71\", edgecolor=\"white\", alpha=0.7, label=\"Sample 20% of small\")\n",
    "axes[1].set_title(\"2. GOSS Sampling\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Gradient Magnitude\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Right: Comparison\n",
    "labels = [\"All Samples\", \"GOSS Samples\"]\n",
    "sizes = [len(gradients), len(large_grads) + len(sampled_small)]\n",
    "colors_pie = [\"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "bars = axes[2].bar(labels, sizes, color=colors_pie, edgecolor=\"black\", linewidth=2)\n",
    "axes[2].set_ylabel(\"Number of Samples\")\n",
    "axes[2].set_title(\"3. Sample Reduction\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, size in zip(bars, sizes):\n",
    "    pct = size / len(gradients) * 100\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20, \n",
    "                f\"{size} ({pct:.0f}%)\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"GOSS: Keep Important Samples, Subsample the Rest\", fontsize=13, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 GOSS reduced training samples from {len(gradients)} to {len(large_grads) + len(sampled_small)}\")\n",
    "print(f\"   That is a {(1 - (len(large_grads) + len(sampled_small))/len(gradients))*100:.0f}% reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 Key Innovation #3: EFB (Exclusive Feature Bundling)\n",
    "\n",
    "For high-dimensional sparse data (like one-hot encoded categoricals):\n",
    "\n",
    "**Insight:** Many features are mutually exclusive (never non-zero together). We can bundle them into one feature!\n",
    "\n",
    "Example: If you one-hot encode \"day of week\" into 7 features, at most ONE is ever 1. EFB bundles them back into 1 feature.\n",
    "\n",
    "Result: Reduces feature dimensions without losing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.4 Native Categorical Feature Support\n",
    "\n",
    "LightGBM can handle categorical features directly - no need to one-hot encode!\n",
    "\n",
    "How it works:\n",
    "1. For each categorical feature, LightGBM finds the **optimal split** (which categories go left vs right)\n",
    "2. Uses an algorithm that considers all possible partitions efficiently\n",
    "3. Result: Better splits than one-hot encoding, and faster\n",
    "\n",
    "To use: Pass categorical column names to the `categorical_feature` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.5 LightGBM Parameters Explained\n",
    "\n",
    "### Tree Structure Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `num_leaves` | Max leaves per tree (KEY param!) | 31 | 15-127 |\n",
    "| `max_depth` | Max tree depth (-1 = unlimited) | -1 | -1 or 5-15 |\n",
    "| `min_data_in_leaf` | Min samples in a leaf | 20 | 10-100 |\n",
    "| `min_sum_hessian_in_leaf` | Like min_child_weight | 1e-3 | 1e-3 to 10 |\n",
    "\n",
    "### Sampling Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `bagging_fraction` | Row sampling (like subsample) | 1.0 | 0.5-1.0 |\n",
    "| `bagging_freq` | How often to bag (0=disabled) | 0 | 1-10 |\n",
    "| `feature_fraction` | Column sampling (like colsample) | 1.0 | 0.5-1.0 |\n",
    "\n",
    "### Regularization Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `reg_alpha` | L1 regularization | 0 | 0-1 |\n",
    "| `reg_lambda` | L2 regularization | 0 | 0-10 |\n",
    "| `min_gain_to_split` | Min gain for split (like gamma) | 0 | 0-1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.6 LightGBM in Practice\n",
    "\n",
    "Let's train LightGBM on California Housing and compare speed with XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train LightGBM with proper settings\n",
    "print(\"Training LightGBM on California Housing...\\n\")\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "lgb_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.early_stopping(50, verbose=False)]\n",
    ")\n",
    "lgb_train_time = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "lgb_mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "lgb_r2 = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f\"\ud83d\udcca LightGBM Results:\")\n",
    "print(f\"   Best iteration: {lgb_model.best_iteration_}\")\n",
    "print(f\"   Training time: {lgb_train_time:.2f}s\")\n",
    "print(f\"   Test RMSE: {lgb_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {lgb_mae:.4f}\")\n",
    "print(f\"   Test R\u00b2: {lgb_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare XGBoost vs LightGBM\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics = [\"RMSE\", \"MAE\", \"R\u00b2\"]\n",
    "xgb_metrics = [rmse, mae, r2]\n",
    "lgb_metrics = [lgb_rmse, lgb_mae, lgb_r2]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, xgb_metrics, width, label=\"XGBoost\", \n",
    "                    color=COLORS[\"XGBoost\"], edgecolor=\"black\")\n",
    "bars2 = axes[0].bar(x + width/2, lgb_metrics, width, label=\"LightGBM\", \n",
    "                    color=COLORS[\"LightGBM\"], edgecolor=\"black\")\n",
    "\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].set_title(\"XGBoost vs LightGBM: Accuracy Metrics\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                    f\"{height:.4f}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "# Training time comparison\n",
    "times = [train_time, lgb_train_time]\n",
    "names = [\"XGBoost\", \"LightGBM\"]\n",
    "colors = [COLORS[\"XGBoost\"], COLORS[\"LightGBM\"]]\n",
    "\n",
    "bars = axes[1].bar(names, times, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "axes[1].set_ylabel(\"Training Time (seconds)\", fontsize=12)\n",
    "axes[1].set_title(\"XGBoost vs LightGBM: Training Speed\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Add time labels and speedup\n",
    "for bar, t in zip(bars, times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f\"{t:.2f}s\", ha=\"center\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "speedup = train_time / lgb_train_time\n",
    "axes[1].text(0.5, max(times) * 0.6, f\"LightGBM is {speedup:.1f}x faster!\", \n",
    "            ha=\"center\", fontsize=14, fontweight=\"bold\", color=\"green\",\n",
    "            transform=axes[1].transData)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 LightGBM is {speedup:.1f}x faster than XGBoost with similar accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.7 LightGBM Parameter Reference\n",
    "\n",
    "| Parameter | LightGBM Name | Description | Recommended Range |\n",
    "|-----------|---------------|-------------|-------------------|\n",
    "| Learning rate | `learning_rate` | Shrinkage per tree | 0.01-0.3 |\n",
    "| Number of trees | `n_estimators` | Boosting rounds | Use early stopping |\n",
    "| Max leaves | `num_leaves` | **KEY PARAM** - leaves per tree | 15-127 |\n",
    "| Max depth | `max_depth` | Tree depth (-1 = no limit) | -1 or 5-15 |\n",
    "| Min data in leaf | `min_child_samples` | Min samples per leaf | 10-100 |\n",
    "| Row sampling | `bagging_fraction` | Subsample ratio | 0.5-1.0 |\n",
    "| Column sampling | `feature_fraction` | Feature ratio | 0.5-1.0 |\n",
    "| L1 regularization | `reg_alpha` | Lasso penalty | 0-1 |\n",
    "| L2 regularization | `reg_lambda` | Ridge penalty | 0-10 |\n",
    "| Boosting type | `boosting_type` | Algorithm | \"gbdt\", \"dart\", \"goss\" |\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: Section 5 - CatBoost Deep Dive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-5\"></a>\n",
    "# Section 5: CatBoost Deep Dive\n",
    "\n",
    "**CatBoost** (Categorical Boosting) was released by Yandex in 2017. Its main innovations focus on:\n",
    "1. **Superior categorical feature handling** - the best of any boosting library\n",
    "2. **Ordered boosting** - a clever technique to reduce overfitting\n",
    "3. **Symmetric trees** - fast inference with built-in regularization\n",
    "\n",
    "Let's understand what makes CatBoost special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 The Categorical Feature Problem\n",
    "\n",
    "Most ML models need numeric inputs. How do we handle categorical features like \"city\" or \"day of week\"?\n",
    "\n",
    "### Common Approaches (and their problems)\n",
    "\n",
    "| Method | How It Works | Problem |\n",
    "|--------|--------------|----------|\n",
    "| **One-Hot Encoding** | Create binary column per category | Explodes dimensionality for high cardinality |\n",
    "| **Label Encoding** | Assign numbers (0, 1, 2...) | Implies false ordering |\n",
    "| **Target Encoding** | Replace with mean target value | **Leaks information** - causes overfitting |\n",
    "\n",
    "Target encoding is powerful but dangerous: if you compute the mean target for \"New York\" using all \"New York\" samples, then use that same mean to predict \"New York\" samples, you are cheating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.2 Key Innovation #1: Ordered Target Statistics\n",
    "\n",
    "CatBoost's solution to target encoding leakage is elegant:\n",
    "\n",
    "**For each sample, compute target statistics using only samples that came BEFORE it** (in a random ordering).\n",
    "\n",
    "Let's walk through an example:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Demonstrate Ordered Target Statistics\n",
    "print(\"=\"*70)\n",
    "print(\"ORDERED TARGET STATISTICS: Avoiding Target Leakage\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example data: predicting price by city\n",
    "data = pd.DataFrame({\n",
    "    \"City\": [\"NYC\", \"LA\", \"NYC\", \"LA\", \"NYC\", \"LA\"],\n",
    "    \"Price\": [500, 300, 550, 280, 480, 320]\n",
    "})\n",
    "\n",
    "print(\"\\n\ud83d\udcca Original Data:\")\n",
    "print(data.to_string(index=False))\n",
    "\n",
    "# Naive target encoding (WRONG - causes leakage)\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\u274c NAIVE TARGET ENCODING (with leakage):\")\n",
    "city_means = data.groupby(\"City\")[\"Price\"].mean()\n",
    "print(f\"   NYC mean: {city_means['NYC']:.0f}\")\n",
    "print(f\"   LA mean: {city_means['LA']:.0f}\")\n",
    "print(\"   Problem: Each NYC sample uses its own value in the mean!\")\n",
    "\n",
    "# CatBoost ordered target statistics (CORRECT)\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"\u2705 CATBOOST ORDERED TARGET STATISTICS:\")\n",
    "print(\"   For each row, only use PREVIOUS rows to compute the encoding:\\n\")\n",
    "\n",
    "# Simulate CatBoost encoding\n",
    "prior = 400  # Global prior (dataset mean)\n",
    "encodings = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    city = data.iloc[i][\"City\"]\n",
    "    \n",
    "    # Get all previous samples of same city\n",
    "    prev_same_city = data.iloc[:i][data.iloc[:i][\"City\"] == city][\"Price\"]\n",
    "    \n",
    "    if len(prev_same_city) == 0:\n",
    "        encoding = prior  # Use global prior if no history\n",
    "        explanation = f\"No previous {city} \u2192 use prior ({prior})\"\n",
    "    else:\n",
    "        encoding = prev_same_city.mean()\n",
    "        explanation = f\"Mean of previous {city} samples: {list(prev_same_city.values)} \u2192 {encoding:.0f}\"\n",
    "    \n",
    "    encodings.append(encoding)\n",
    "    print(f\"   Row {i} ({city}): {explanation}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Key Insight: Each sample is encoded using ONLY past information - no leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.3 Key Innovation #2: Ordered Boosting\n",
    "\n",
    "CatBoost applies the same \"ordered\" principle to boosting itself.\n",
    "\n",
    "**The problem with standard boosting:**\n",
    "When computing residuals for sample i, we use a model trained on all samples including i. This is a subtle form of information leakage.\n",
    "\n",
    "**CatBoost's solution:**\n",
    "1. Create multiple random orderings of the data\n",
    "2. For each sample, compute residuals using a model trained only on previous samples in that ordering\n",
    "3. Average across orderings for stability\n",
    "\n",
    "Result: More honest error estimation \u2192 less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.4 Key Innovation #3: Symmetric (Oblivious) Trees\n",
    "\n",
    "CatBoost uses **symmetric trees** by default - a very different structure from XGBoost/LightGBM:\n",
    "\n",
    "**In a symmetric tree:** All nodes at the same depth use the SAME split condition.\n",
    "\n",
    "Let's visualize the difference:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize symmetric vs asymmetric trees\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# LEFT: Normal asymmetric tree\n",
    "ax = axes[0]\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Normal Tree (XGBoost/LightGBM)\\nDifferent splits at each node\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "def draw_node_box(ax, x, y, text, color):\n",
    "    ax.add_patch(plt.Rectangle((x-0.3, y-0.15), 0.6, 0.3, \n",
    "                                facecolor=color, edgecolor=\"black\", linewidth=2))\n",
    "    ax.text(x, y, text, ha=\"center\", va=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "# Root\n",
    "draw_node_box(ax, 2, 3.5, \"X1 < 5\", \"#3498db\")\n",
    "\n",
    "# Level 1\n",
    "draw_node_box(ax, 1, 2.5, \"X2 < 3\", \"#e74c3c\")\n",
    "draw_node_box(ax, 3, 2.5, \"X3 < 7\", \"#e74c3c\")\n",
    "ax.plot([2, 1], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "ax.plot([2, 3], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Level 2 (different splits!)\n",
    "draw_node_box(ax, 0.5, 1.5, \"X4 < 2\", \"#2ecc71\")\n",
    "draw_node_box(ax, 1.5, 1.5, \"X1 < 3\", \"#2ecc71\")\n",
    "draw_node_box(ax, 2.5, 1.5, \"X2 < 8\", \"#2ecc71\")\n",
    "draw_node_box(ax, 3.5, 1.5, \"X5 < 1\", \"#2ecc71\")\n",
    "for x1, x2 in [(1, 0.5), (1, 1.5), (3, 2.5), (3, 3.5)]:\n",
    "    ax.plot([x1, x2], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "\n",
    "# RIGHT: Symmetric tree (CatBoost)\n",
    "ax = axes[1]\n",
    "ax.set_xlim(0, 4)\n",
    "ax.set_ylim(0, 4)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Symmetric Tree (CatBoost)\\nSAME split at each depth\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Root\n",
    "draw_node_box(ax, 2, 3.5, \"X1 < 5\", \"#3498db\")\n",
    "\n",
    "# Level 1 - SAME split\n",
    "draw_node_box(ax, 1, 2.5, \"X2 < 3\", \"#f39c12\")\n",
    "draw_node_box(ax, 3, 2.5, \"X2 < 3\", \"#f39c12\")\n",
    "ax.plot([2, 1], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "ax.plot([2, 3], [3.35, 2.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Level 2 - SAME split\n",
    "draw_node_box(ax, 0.5, 1.5, \"X3 < 7\", \"#9b59b6\")\n",
    "draw_node_box(ax, 1.5, 1.5, \"X3 < 7\", \"#9b59b6\")\n",
    "draw_node_box(ax, 2.5, 1.5, \"X3 < 7\", \"#9b59b6\")\n",
    "draw_node_box(ax, 3.5, 1.5, \"X3 < 7\", \"#9b59b6\")\n",
    "for x1, x2 in [(1, 0.5), (1, 1.5), (3, 2.5), (3, 3.5)]:\n",
    "    ax.plot([x1, x2], [2.35, 1.65], \"k-\", linewidth=2)\n",
    "\n",
    "# Highlight the symmetry\n",
    "ax.add_patch(plt.Rectangle((0.6, 2.3), 2.8, 0.4, facecolor=\"none\", \n",
    "                           edgecolor=\"red\", linewidth=3, linestyle=\"--\"))\n",
    "ax.text(2, 2.85, \"Same condition!\", color=\"red\", fontsize=10, ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"Symmetric vs Asymmetric Tree Structures\", fontsize=13, fontweight=\"bold\", y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Symmetric Tree Benefits:\")\n",
    "print(\"   1. Faster inference (can use bit operations)\")\n",
    "print(\"   2. Built-in regularization (simpler structure)\")\n",
    "print(\"   3. Less prone to overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.5 CatBoost Parameters Explained\n",
    "\n",
    "### Tree Structure Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `depth` | Tree depth (KEY param for symmetric trees) | 6 | 4-10 |\n",
    "| `min_data_in_leaf` | Min samples in leaf | 1 | 1-100 |\n",
    "| `grow_policy` | Tree structure | \"SymmetricTree\" | Also: \"Depthwise\", \"Lossguide\" |\n",
    "\n",
    "### Regularization Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Typical Range |\n",
    "|-----------|--------------|---------|---------------|\n",
    "| `l2_leaf_reg` | L2 regularization (KEY param) | 3 | 1-10 |\n",
    "| `random_strength` | Regularization via randomness | 1 | 0-10 |\n",
    "| `bagging_temperature` | Controls bootstrap aggression | 1 | 0-10 |\n",
    "\n",
    "### Categorical Parameters\n",
    "\n",
    "| Parameter | What It Does | Default | Notes |\n",
    "|-----------|--------------|---------|-------|\n",
    "| `cat_features` | List of categorical column indices/names | Auto-detect | Always specify explicitly |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.6 CatBoost in Practice\n",
    "\n",
    "Let's train CatBoost on California Housing:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train CatBoost\n",
    "print(\"Training CatBoost on California Housing...\\n\")\n",
    "\n",
    "# CatBoost model\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_strength=1,\n",
    "    bagging_temperature=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "cat_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=(X_val, y_val),\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "cat_train_time = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred_cat = cat_model.predict(X_test)\n",
    "cat_rmse = np.sqrt(mean_squared_error(y_test, y_pred_cat))\n",
    "cat_mae = mean_absolute_error(y_test, y_pred_cat)\n",
    "cat_r2 = r2_score(y_test, y_pred_cat)\n",
    "\n",
    "print(f\"\ud83d\udcca CatBoost Results:\")\n",
    "print(f\"   Best iteration: {cat_model.best_iteration_}\")\n",
    "print(f\"   Training time: {cat_train_time:.2f}s\")\n",
    "print(f\"   Test RMSE: {cat_rmse:.4f}\")\n",
    "print(f\"   Test MAE: {cat_mae:.4f}\")\n",
    "print(f\"   Test R\u00b2: {cat_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare all three frameworks so far\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE Comparison\n",
    "frameworks = [\"XGBoost\", \"LightGBM\", \"CatBoost\"]\n",
    "rmses = [rmse, lgb_rmse, cat_rmse]\n",
    "colors = [COLORS[\"XGBoost\"], COLORS[\"LightGBM\"], COLORS[\"CatBoost\"]]\n",
    "\n",
    "bars = axes[0].bar(frameworks, rmses, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "axes[0].set_ylabel(\"RMSE (lower is better)\", fontsize=12)\n",
    "axes[0].set_title(\"Framework Comparison: Accuracy\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "for bar, r in zip(bars, rmses):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "                f\"{r:.4f}\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Training Time Comparison\n",
    "times = [train_time, lgb_train_time, cat_train_time]\n",
    "\n",
    "bars = axes[1].bar(frameworks, times, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "axes[1].set_ylabel(\"Training Time (seconds)\", fontsize=12)\n",
    "axes[1].set_title(\"Framework Comparison: Speed\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "for bar, t in zip(bars, times):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f\"{t:.2f}s\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Summary so far:\")\n",
    "print(f\"   Best RMSE: {frameworks[np.argmin(rmses)]} ({min(rmses):.4f})\")\n",
    "print(f\"   Fastest: {frameworks[np.argmin(times)]} ({min(times):.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.7 CatBoost Parameter Reference\n",
    "\n",
    "| Parameter | CatBoost Name | Description | Recommended Range |\n",
    "|-----------|---------------|-------------|-------------------|\n",
    "| Learning rate | `learning_rate` | Shrinkage per tree | 0.01-0.3 |\n",
    "| Number of trees | `iterations` | Boosting rounds | Use early stopping |\n",
    "| Tree depth | `depth` | Tree depth (symmetric) | 4-10 |\n",
    "| L2 regularization | `l2_leaf_reg` | Ridge penalty | 1-10 |\n",
    "| Random strength | `random_strength` | Adds randomness | 0-10 |\n",
    "| Bagging temp | `bagging_temperature` | Bootstrap control | 0-10 |\n",
    "| Categorical features | `cat_features` | Categorical columns | Specify explicitly |\n",
    "| Tree structure | `grow_policy` | Growth strategy | \"SymmetricTree\" |\n",
    "\n",
    "---\n",
    "\n",
    "**Next up: Section 6 - Scikit-learn Gradient Boosting (brief) and Section 7 - Head-to-Head Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-6\"></a>\n",
    "# Section 6: Scikit-learn's Gradient Boosting\n",
    "\n",
    "Sklearn has two gradient boosting implementations:\n",
    "1. `GradientBoostingRegressor` - Original implementation (slower)\n",
    "2. `HistGradientBoostingRegressor` - Histogram-based (like LightGBM)\n",
    "\n",
    "We'll focus on the latter as it's much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Sklearn's Gradient Boosting\n",
    "\n",
    "| Use Sklearn When... | Use XGBoost/LightGBM/CatBoost When... |\n",
    "|---------------------|---------------------------------------|\n",
    "| You want sklearn pipeline integration | You need maximum performance |\n",
    "| You prefer simplicity (fewer params) | You have categorical features |\n",
    "| You are prototyping quickly | You need GPU support |\n",
    "| You want minimal dependencies | You are competing on Kaggle |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train sklearn HistGradientBoostingRegressor\n",
    "print(\"Training sklearn HistGradientBoostingRegressor...\\n\")\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "hgbr = HistGradientBoostingRegressor(\n",
    "    max_iter=1000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=None,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=20,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=50,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "hgbr.fit(X_train, y_train)\n",
    "sklearn_train_time = time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred_sklearn = hgbr.predict(X_test)\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(y_test, y_pred_sklearn))\n",
    "sklearn_r2 = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"\ud83d\udcca Sklearn HGBR Results:\")\n",
    "print(f\"   Training time: {sklearn_train_time:.2f}s\")\n",
    "print(f\"   Test RMSE: {sklearn_rmse:.4f}\")\n",
    "print(f\"   Test R\u00b2: {sklearn_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-7\"></a>\n",
    "# Section 7: Head-to-Head Comparison\n",
    "\n",
    "This is the centerpiece - a fair comparison of all frameworks on the same data with the same evaluation protocol."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a standardized evaluation function\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, name):\n",
    "    \"\"\"Train and evaluate a model, returning metrics and timing.\"\"\"\n",
    "    \n",
    "    # Training time\n",
    "    start = time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    # Predictions\n",
    "    start = time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_time = time() - start\n",
    "    \n",
    "    # Metrics\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"Train Time\": train_time,\n",
    "        \"Pred Time\": pred_time\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined \u2713\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the full comparison\n",
    "print(\"=\"*70)\n",
    "print(\"HEAD-TO-HEAD COMPARISON: All Frameworks on California Housing\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Models with consistent hyperparameters\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeRegressor(max_depth=7, random_state=RANDOM_STATE),\n",
    "    \n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=None, random_state=RANDOM_STATE, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, max_depth=5, \n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, verbosity=0\n",
    "    ),\n",
    "    \n",
    "    \"LightGBM\": lgb.LGBMRegressor(\n",
    "        n_estimators=500, learning_rate=0.05, num_leaves=31,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE, verbosity=-1\n",
    "    ),\n",
    "    \n",
    "    \"CatBoost\": CatBoostRegressor(\n",
    "        iterations=500, learning_rate=0.05, depth=6,\n",
    "        random_state=RANDOM_STATE, verbose=0\n",
    "    ),\n",
    "    \n",
    "    \"Sklearn HGBR\": HistGradientBoostingRegressor(\n",
    "        max_iter=500, learning_rate=0.05, max_leaf_nodes=31,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    result = evaluate_model(model, X_train, y_train, X_test, y_test, name)\n",
    "    results.append(result)\n",
    "    print(f\"   RMSE: {result['RMSE']:.4f}, Time: {result['Train Time']:.2f}s\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\u2713 All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Format the dataframe for display\n",
    "display_df = results_df.copy()\n",
    "display_df[\"RMSE\"] = display_df[\"RMSE\"].round(4)\n",
    "display_df[\"MAE\"] = display_df[\"MAE\"].round(4)\n",
    "display_df[\"R2\"] = display_df[\"R2\"].round(4)\n",
    "display_df[\"Train Time\"] = display_df[\"Train Time\"].round(2).astype(str) + \"s\"\n",
    "display_df[\"Pred Time\"] = (display_df[\"Pred Time\"] * 1000).round(1).astype(str) + \"ms\"\n",
    "\n",
    "# Sort by RMSE\n",
    "display_df = display_df.sort_values(\"RMSE\")\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Find winners\n",
    "best_rmse_model = results_df.loc[results_df[\"RMSE\"].idxmin(), \"Model\"]\n",
    "fastest_model = results_df.loc[results_df[\"Train Time\"].idxmin(), \"Model\"]\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Accuracy: {best_rmse_model}\")\n",
    "print(f\"\u26a1 Fastest Training: {fastest_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Color mapping\n",
    "model_colors = {\n",
    "    \"Decision Tree\": COLORS[\"DecisionTree\"],\n",
    "    \"Random Forest\": COLORS[\"RandomForest\"],\n",
    "    \"XGBoost\": COLORS[\"XGBoost\"],\n",
    "    \"LightGBM\": COLORS[\"LightGBM\"],\n",
    "    \"CatBoost\": COLORS[\"CatBoost\"],\n",
    "    \"Sklearn HGBR\": COLORS[\"sklearn_HGBR\"]\n",
    "}\n",
    "\n",
    "# 1. RMSE Comparison\n",
    "ax = axes[0, 0]\n",
    "sorted_df = results_df.sort_values(\"RMSE\")\n",
    "colors = [model_colors[m] for m in sorted_df[\"Model\"]]\n",
    "bars = ax.barh(sorted_df[\"Model\"], sorted_df[\"RMSE\"], color=colors, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"RMSE (lower is better)\")\n",
    "ax.set_title(\"Model Accuracy Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "ax.invert_yaxis()\n",
    "for bar, rmse in zip(bars, sorted_df[\"RMSE\"]):\n",
    "    ax.text(rmse + 0.01, bar.get_y() + bar.get_height()/2, f\"{rmse:.4f}\", \n",
    "           va=\"center\", fontsize=10)\n",
    "\n",
    "# 2. Training Time Comparison\n",
    "ax = axes[0, 1]\n",
    "sorted_df = results_df.sort_values(\"Train Time\")\n",
    "colors = [model_colors[m] for m in sorted_df[\"Model\"]]\n",
    "bars = ax.barh(sorted_df[\"Model\"], sorted_df[\"Train Time\"], color=colors, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Training Time (seconds)\")\n",
    "ax.set_title(\"Training Speed Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "ax.invert_yaxis()\n",
    "for bar, t in zip(bars, sorted_df[\"Train Time\"]):\n",
    "    ax.text(t + 0.05, bar.get_y() + bar.get_height()/2, f\"{t:.2f}s\", \n",
    "           va=\"center\", fontsize=10)\n",
    "\n",
    "# 3. R\u00b2 Comparison\n",
    "ax = axes[1, 0]\n",
    "sorted_df = results_df.sort_values(\"R2\", ascending=False)\n",
    "colors = [model_colors[m] for m in sorted_df[\"Model\"]]\n",
    "bars = ax.barh(sorted_df[\"Model\"], sorted_df[\"R2\"], color=colors, edgecolor=\"black\")\n",
    "ax.set_xlabel(\"R\u00b2 Score (higher is better)\")\n",
    "ax.set_title(\"R\u00b2 Score Comparison\", fontsize=12, fontweight=\"bold\")\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlim(0.5, 1.0)\n",
    "\n",
    "# 4. Accuracy vs Speed Tradeoff\n",
    "ax = axes[1, 1]\n",
    "for _, row in results_df.iterrows():\n",
    "    ax.scatter(row[\"Train Time\"], row[\"RMSE\"], s=200, \n",
    "              c=model_colors[row[\"Model\"]], edgecolors=\"black\", linewidth=2)\n",
    "    ax.annotate(row[\"Model\"], (row[\"Train Time\"], row[\"RMSE\"]),\n",
    "               xytext=(5, 5), textcoords=\"offset points\", fontsize=9)\n",
    "ax.set_xlabel(\"Training Time (seconds)\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_title(\"Accuracy vs Speed Tradeoff\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Ideal corner\n",
    "ax.annotate(\"\u2190 Ideal\\n(fast & accurate)\", xy=(0.5, 0.47), fontsize=10, \n",
    "           color=\"green\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.1 Parameter Mapping Table\n",
    "\n",
    "The same concept has different names across frameworks:\n",
    "\n",
    "| Concept | XGBoost | LightGBM | CatBoost | Sklearn HGBR |\n",
    "|---------|---------|----------|----------|---------------|\n",
    "| Learning rate | `learning_rate` | `learning_rate` | `learning_rate` | `learning_rate` |\n",
    "| Number of trees | `n_estimators` | `n_estimators` | `iterations` | `max_iter` |\n",
    "| Tree depth | `max_depth` | `max_depth` | `depth` | `max_depth` |\n",
    "| Max leaves | `max_leaves` | `num_leaves` | - | `max_leaf_nodes` |\n",
    "| Min samples/leaf | `min_child_weight` | `min_child_samples` | `min_data_in_leaf` | `min_samples_leaf` |\n",
    "| Row subsampling | `subsample` | `bagging_fraction` | `subsample` | - |\n",
    "| Col subsampling | `colsample_bytree` | `feature_fraction` | `rsm` | - |\n",
    "| L1 regularization | `reg_alpha` | `reg_alpha` | - | - |\n",
    "| L2 regularization | `reg_lambda` | `reg_lambda` | `l2_leaf_reg` | - |\n",
    "| Min split gain | `gamma` | `min_split_gain` | - | `min_impurity_decrease` |\n",
    "| Categorical support | Limited | Native | **Best** | Native |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.2 Architectural Differences Summary\n",
    "\n",
    "| Aspect | XGBoost | LightGBM | CatBoost |\n",
    "|--------|---------|----------|----------|\n",
    "| **Tree Growth** | Level-wise | Leaf-wise | Symmetric |\n",
    "| **Split Finding** | Histogram | Histogram + GOSS + EFB | Histogram |\n",
    "| **Categorical Handling** | One-hot (default) | Native (good) | Native (best) |\n",
    "| **Missing Values** | Learns optimal direction | Learns optimal direction | Learns optimal direction |\n",
    "| **Key Innovation** | Regularized objective | Speed optimizations | Ordered boosting |\n",
    "| **Best For** | General purpose | Large datasets, speed | Categorical features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7.3 Which Framework Should I Use?\n",
    "\n",
    "```\n",
    "START\n",
    "  \u2502\n",
    "  \u251c\u2500\u2500 Do you have many categorical features?\n",
    "  \u2502   \u2514\u2500\u2500 YES \u2192 CatBoost\n",
    "  \u2502\n",
    "  \u251c\u2500\u2500 Is your dataset very large (>1M rows)?\n",
    "  \u2502   \u2514\u2500\u2500 YES \u2192 LightGBM (fastest)\n",
    "  \u2502\n",
    "  \u251c\u2500\u2500 Do you need sklearn pipeline integration?\n",
    "  \u2502   \u2514\u2500\u2500 YES \u2192 Sklearn HistGradientBoostingRegressor\n",
    "  \u2502\n",
    "  \u251c\u2500\u2500 Are you iterating quickly and need simplicity?\n",
    "  \u2502   \u2514\u2500\u2500 YES \u2192 LightGBM (good defaults)\n",
    "  \u2502\n",
    "  \u2514\u2500\u2500 Default choice \u2192 XGBoost (mature, well-documented)\n",
    "```\n",
    "\n",
    "**In practice:** Most experienced practitioners start with LightGBM for speed during experimentation, then compare with XGBoost and CatBoost before finalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-8\"></a>\n",
    "# Section 8: Feature Importance & Explainability\n",
    "\n",
    "Understanding WHY a model makes predictions is crucial for trust, debugging, and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Built-in Feature Importance\n",
    "\n",
    "Tree models provide feature importance scores. Two main types:\n",
    "\n",
    "| Type | How It Works | Pros | Cons |\n",
    "|------|--------------|------|------|\n",
    "| **Split-based** | Count how often a feature is used | Fast | Biased toward high-cardinality |\n",
    "| **Gain-based** | Sum of loss reduction from splits | More accurate | Still can be misleading |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare feature importance across frameworks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_imp = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values()\n",
    "axes[0].barh(xgb_imp.index, xgb_imp.values, color=COLORS[\"XGBoost\"])\n",
    "axes[0].set_title(\"XGBoost Feature Importance\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Importance\")\n",
    "\n",
    "# LightGBM importance\n",
    "lgb_imp = pd.Series(lgb_model.feature_importances_, index=feature_names).sort_values()\n",
    "axes[1].barh(lgb_imp.index, lgb_imp.values, color=COLORS[\"LightGBM\"])\n",
    "axes[1].set_title(\"LightGBM Feature Importance\", fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Importance\")\n",
    "\n",
    "# CatBoost importance\n",
    "cat_imp = pd.Series(cat_model.feature_importances_, index=feature_names).sort_values()\n",
    "axes[2].barh(cat_imp.index, cat_imp.values, color=COLORS[\"CatBoost\"])\n",
    "axes[2].set_title(\"CatBoost Feature Importance\", fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Importance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\U0001f4a1 Notice: All frameworks agree on the most important features (MedInc, AveOccup, etc.)\")\n",
    "print(\"   but the exact rankings may differ slightly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 SHAP Values: The Gold Standard\n",
    "\n",
    "**SHAP** (SHapley Additive exPlanations) provides theoretically grounded feature importance.\n",
    "\n",
    "Based on game theory: \"How much does each feature contribute to pushing the prediction away from the average?\"\n",
    "\n",
    "Key benefits:\n",
    "- **Local explanations**: Understand individual predictions\n",
    "- **Global explanations**: Understand overall feature importance\n",
    "- **Consistent**: Satisfies mathematical fairness properties"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate SHAP values for XGBoost model\n",
    "print(\"Calculating SHAP values (this may take a moment)...\\n\")\n",
    "\n",
    "# Create explainer\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "X_sample = X_test.iloc[:500]\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"SHAP values calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Beeswarm)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"violin\", show=False)\n",
    "plt.title(\"SHAP Summary: How Each Feature Affects Predictions\", fontsize=12, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\U0001f4a1 How to read this plot:\")\n",
    "print(\"   - Features sorted by importance (top = most important)\")\n",
    "print(\"   - Color: red = high feature value, blue = low\")\n",
    "print(\"   - X-axis: impact on prediction (positive = higher price)\")\n",
    "print(\"   - Example: High MedInc (red) pushes predictions UP (right)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot - Explain a single prediction\n",
    "print(\"Explaining a single prediction...\\n\")\n",
    "\n",
    "# Pick an interesting sample\n",
    "sample_idx = 42\n",
    "sample = X_sample.iloc[sample_idx:sample_idx+1]\n",
    "actual_value = y_test[sample_idx]\n",
    "predicted_value = xgb_model.predict(sample)[0]\n",
    "\n",
    "print(f\"Sample {sample_idx}:\")\n",
    "print(f\"   Actual price: ${actual_value*100:.0f}k\")\n",
    "print(f\"   Predicted price: ${predicted_value*100:.0f}k\")\n",
    "\n",
    "# Waterfall plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_values[sample_idx],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_sample.iloc[sample_idx],\n",
    "    feature_names=feature_names\n",
    "), show=False)\n",
    "plt.title(f\"SHAP Waterfall: Why This House is Predicted at ${predicted_value*100:.0f}k\", \n",
    "         fontsize=11, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\U0001f4a1 This shows how each feature pushed the prediction from the baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-9\"></a>\n",
    "# Section 9: Practical Tips & Common Pitfalls\n",
    "\n",
    "Hard-won advice from real-world experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Starting a New Project\n",
    "\n",
    "**Recommended workflow:**\n",
    "\n",
    "1. **Start with LightGBM** - fastest iteration speed\n",
    "2. **Use early stopping** - never hardcode n_estimators\n",
    "3. **Establish a baseline** with default parameters\n",
    "4. **Compare frameworks** once you have a working pipeline\n",
    "5. **Tune hyperparameters** only after validating the approach\n",
    "\n",
    "## 9.2 Always Use Early Stopping\n",
    "\n",
    "```python\n",
    "# \u274c WRONG: Guessing n_estimators\n",
    "model = XGBRegressor(n_estimators=500)\n",
    "\n",
    "# \u2705 CORRECT: Early stopping finds optimal\n",
    "model = XGBRegressor(\n",
    "    n_estimators=10000,  # High ceiling\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Learning Rate & Trees are Coupled\n",
    "\n",
    "| Learning Rate | Trees Needed | Quality | Speed |\n",
    "|---------------|--------------|---------|-------|\n",
    "| 0.3 | ~100 | Good | Fast |\n",
    "| 0.1 | ~300 | Better | Medium |\n",
    "| 0.05 | ~600 | Even Better | Slower |\n",
    "| 0.01 | ~2000+ | Often Best | Slow |\n",
    "\n",
    "**Rule of thumb:** Start with 0.1, try 0.05 and 0.01 with more trees.\n",
    "\n",
    "## 9.4 num_leaves vs max_depth (LightGBM)\n",
    "\n",
    "Common mistake: setting `num_leaves=128` with `max_depth=5`.\n",
    "\n",
    "**Remember:** `num_leaves <= 2^max_depth`\n",
    "\n",
    "| max_depth | Max num_leaves |\n",
    "|-----------|---------------|\n",
    "| 5 | 32 |\n",
    "| 6 | 64 |\n",
    "| 7 | 128 |\n",
    "| 8 | 256 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Reading Learning Curves\n",
    "\n",
    "| Curve Shape | Diagnosis | Action |\n",
    "|-------------|-----------|--------|\n",
    "| Both dropping, small gap | Healthy | Continue |\n",
    "| Train dropping, val flat/rising | Overfitting | Reduce complexity, add regularization |\n",
    "| Both flat and high | Underfitting | Increase complexity |\n",
    "| Large gap between curves | High variance | Add regularization, more data |\n",
    "\n",
    "## 9.6 Categorical Features Ranking\n",
    "\n",
    "For native categorical handling:\n",
    "\n",
    "**CatBoost > LightGBM > XGBoost**\n",
    "\n",
    "- CatBoost: Best handling, no prep needed\n",
    "- LightGBM: Good native support\n",
    "- XGBoost: Use one-hot encoding (native support experimental)\n",
    "\n",
    "## 9.7 Hyperparameter Tuning Priority\n",
    "\n",
    "1. **learning_rate** (most important)\n",
    "2. **n_estimators** (via early stopping)\n",
    "3. **max_depth / num_leaves**\n",
    "4. **min_child_weight / min_samples_leaf**\n",
    "5. **subsample / colsample_bytree**\n",
    "6. **regularization** (alpha, lambda)\n",
    "\n",
    "Don't tune everything at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.8 When NOT to Use Tree-Based Models\n",
    "\n",
    "| Situation | Better Alternative |\n",
    "|-----------|--------------------|\n",
    "| Images, audio, video | Deep learning (CNNs) |\n",
    "| Text, NLP | Transformers, embeddings |\n",
    "| Very high-dimensional sparse data | Linear models, factorization |\n",
    "| Tiny datasets (<100 rows) | Simple models, cross-validation |\n",
    "| Need smooth/continuous predictions | Neural networks, GPs |\n",
    "| Real-time, ultra-low latency | Linear models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-10\"></a>\n",
    "# Section 10: Quick Reference Cheat Sheet\n",
    "\n",
    "## Framework Comparison at a Glance\n",
    "\n",
    "| Framework | Tree Growth | Speed | Categorical | Best For |\n",
    "|-----------|-------------|-------|-------------|----------|\n",
    "| XGBoost | Level-wise | Medium | Limited | General purpose |\n",
    "| LightGBM | Leaf-wise | **Fastest** | Good | Large datasets |\n",
    "| CatBoost | Symmetric | Medium | **Best** | Categorical features |\n",
    "\n",
    "## Top 5 Parameters to Tune\n",
    "\n",
    "| XGBoost | LightGBM | CatBoost |\n",
    "|---------|----------|----------|\n",
    "| learning_rate | learning_rate | learning_rate |\n",
    "| max_depth | num_leaves | depth |\n",
    "| min_child_weight | min_child_samples | l2_leaf_reg |\n",
    "| subsample | bagging_fraction | random_strength |\n",
    "| colsample_bytree | feature_fraction | bagging_temperature |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Parameter Recipes\n",
    "\n",
    "### Fast Experimentation\n",
    "```python\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31\n",
    ")\n",
    "```\n",
    "\n",
    "### Production Quality\n",
    "```python\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.02,\n",
    "    num_leaves=63,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "```\n",
    "\n",
    "### Anti-Overfitting\n",
    "```python\n",
    "model = LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=15,  # Shallow\n",
    "    min_child_samples=50,  # High\n",
    "    subsample=0.6,  # Aggressive\n",
    "    reg_lambda=10.0  # Strong\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"section-11\"></a>\n",
    "# Section 11: Classification Example (Bonus)\n",
    "\n",
    "Let's verify the frameworks work similarly on classification with the Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y_cancer = cancer.target\n",
    "\n",
    "# Split\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"Breast Cancer Dataset (Classification)\")\n",
    "print(f\"Training samples: {len(X_train_c)}\")\n",
    "print(f\"Test samples: {len(X_test_c)}\")\n",
    "print(f\"Features: {X_cancer.shape[1]}\")\n",
    "print(f\"Classes: Malignant (0) vs Benign (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, verbosity=0, use_label_encoder=False),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, random_state=RANDOM_STATE, verbosity=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=100, random_state=RANDOM_STATE, verbose=0)\n",
    "}\n",
    "\n",
    "# Evaluate\n",
    "clf_results = []\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_c, y_train_c)\n",
    "    y_pred = clf.predict(X_test_c)\n",
    "    y_prob = clf.predict_proba(X_test_c)[:, 1]\n",
    "    \n",
    "    clf_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test_c, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_test_c, y_prob),\n",
    "        \"F1\": f1_score(y_test_c, y_pred)\n",
    "    })\n",
    "\n",
    "clf_df = pd.DataFrame(clf_results).sort_values(\"AUC-ROC\", ascending=False)\n",
    "print(\"\\nClassification Results:\")\n",
    "print(clf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ROC Curves comparison\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = [COLORS[\"RandomForest\"], COLORS[\"XGBoost\"], COLORS[\"LightGBM\"], COLORS[\"CatBoost\"]]\n",
    "\n",
    "for (name, clf), color in zip(classifiers.items(), colors):\n",
    "    y_prob = clf.predict_proba(X_test_c)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test_c, y_prob)\n",
    "    auc = roc_auc_score(y_test_c, y_prob)\n",
    "    plt.plot(fpr, tpr, color=color, linewidth=2.5, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curves: Classification Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\U0001f4a1 All frameworks achieve excellent classification performance.\")\n",
    "print(\"   The ranking is similar to regression - small differences between boosting frameworks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"references\"></a>\n",
    "# References & Further Reading\n",
    "\n",
    "## Official Documentation\n",
    "\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [LightGBM Documentation](https://lightgbm.readthedocs.io/)\n",
    "- [CatBoost Documentation](https://catboost.ai/docs/)\n",
    "- [SHAP Documentation](https://shap.readthedocs.io/)\n",
    "\n",
    "## Key Papers\n",
    "\n",
    "- **XGBoost:** Chen & Guestrin (2016) - \"XGBoost: A Scalable Tree Boosting System\"\n",
    "- **LightGBM:** Ke et al. (2017) - \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\"\n",
    "- **CatBoost:** Prokhorenkova et al. (2018) - \"CatBoost: unbiased boosting with categorical features\"\n",
    "- **SHAP:** Lundberg & Lee (2017) - \"A Unified Approach to Interpreting Model Predictions\"\n",
    "\n",
    "## Further Learning\n",
    "\n",
    "- **StatQuest with Josh Starmer** - Excellent YouTube explanations of tree algorithms\n",
    "- **Kaggle Learn** - Hands-on micro-courses on XGBoost\n",
    "- **fast.ai** - Practical deep learning course (includes tabular methods)\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed The Definitive Guide to Decision Trees & Gradient Boosting. You now understand:\n",
    "\n",
    "- How decision trees make splits and why they overfit\n",
    "- The difference between bagging and boosting\n",
    "- What makes XGBoost, LightGBM, and CatBoost unique\n",
    "- How to compare and choose between frameworks\n",
    "- How to explain predictions with SHAP\n",
    "- Practical tips for real-world usage\n",
    "\n",
    "**Happy modeling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}